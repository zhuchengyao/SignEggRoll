#!/usr/bin/env python3
"""
ASL Text-to-Poseé¡¹ç›®å¿«é€Ÿæµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯æ•´ä¸ªæµç¨‹æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import os
import sys
import torch
import subprocess
import shutil
from pathlib import Path

def check_dependencies():
    """æ£€æŸ¥ä¾èµ–æ˜¯å¦å®‰è£…"""
    print("ğŸ“¦ æ£€æŸ¥ä¾èµ–...")
    
    required_packages = [
        'torch', 'torchvision', 'transformers', 
        'numpy', 'matplotlib', 'json'
    ]
    
    missing_packages = []
    
    for package in required_packages:
        try:
            if package == 'json':
                import json
            else:
                __import__(package)
            print(f"âœ… {package}")
        except ImportError:
            print(f"âŒ {package}")
            missing_packages.append(package)
    
    if missing_packages:
        print(f"\nç¼ºå°‘ä»¥ä¸‹ä¾èµ–åŒ…ï¼š{missing_packages}")
        print("è¯·è¿è¡Œ: pip install -r requirements_text2video.txt")
        return False
    
    print("âœ… æ‰€æœ‰ä¾èµ–æ£€æŸ¥é€šè¿‡")
    return True

def check_asl_data():
    """æ£€æŸ¥ASLæ•°æ®æ ¼å¼"""
    print("\nğŸ” æ£€æŸ¥ASLæ•°æ®æ ¼å¼...")
    
    # æŸ¥æ‰¾ASLæ•°æ®ç›®å½•
    possible_dirs = [
        "datasets/signllm_training_data/ASL/dev",
        "./datasets/signllm_training_data/ASL/dev", 
        "./asl_data",
        "./datasets",
        "./data",
        "."
    ]
    
    asl_dirs = []
    for base_dir in possible_dirs:
        if os.path.exists(base_dir):
            # å¦‚æœæ˜¯signllmæ•°æ®ç›®å½•ï¼Œç›´æ¥æ£€æŸ¥æ˜¯å¦æœ‰dev_*å­ç›®å½•
            if "signllm_training_data" in base_dir:
                dev_dirs = [d for d in os.listdir(base_dir) if d.startswith('dev_')]
                if dev_dirs:
                    # æ£€æŸ¥å‰å‡ ä¸ªç›®å½•æ˜¯å¦æœ‰pose.jsonå’Œtext.txt
                    for dev_dir in dev_dirs[:3]:
                        dev_path = os.path.join(base_dir, dev_dir)
                        pose_file = os.path.join(dev_path, 'pose.json')
                        text_file = os.path.join(dev_path, 'text.txt')
                        if os.path.exists(pose_file) and os.path.exists(text_file):
                            asl_dirs.append(dev_path)
                    if asl_dirs:
                        print(f"âœ… æ‰¾åˆ° {len(dev_dirs)} ä¸ªASLæ•°æ®ç›®å½•")
                        return base_dir  # è¿”å›çˆ¶ç›®å½•
            else:
                # åŸæœ‰çš„æ£€æŸ¥é€»è¾‘
                for item in os.listdir(base_dir):
                    item_path = os.path.join(base_dir, item)
                    if os.path.isdir(item_path) and item.startswith('dev_'):
                        pose_file = os.path.join(item_path, 'pose.json')
                        text_file = os.path.join(item_path, 'text.txt')
                        if os.path.exists(pose_file) and os.path.exists(text_file):
                            asl_dirs.append(item_path)
                
                if asl_dirs:
                    print(f"âœ… æ‰¾åˆ° {len(asl_dirs)} ä¸ªASLæ•°æ®ç›®å½•")
                    return base_dir  # è¿”å›çˆ¶ç›®å½•
    
    print("âŒ æœªæ‰¾åˆ°ASLæ•°æ®")
    print("è¯·ç¡®ä¿æ•°æ®ç›®å½•åŒ…å«dev_*å­ç›®å½•ï¼Œæ¯ä¸ªå­ç›®å½•æœ‰pose.jsonå’Œtext.txtæ–‡ä»¶")
    return None

def test_data_loading():
    """æµ‹è¯•æ•°æ®åŠ è½½"""
    print("\nğŸ“Š æµ‹è¯•æ•°æ®åŠ è½½...")
    
    try:
        from video_dataset import ASLTextPoseDataset
        
        # æŸ¥æ‰¾æ•°æ®ç›®å½•
        data_dir = check_asl_data()
        if not data_dir:
            return False
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = ASLTextPoseDataset(
            data_dir=data_dir,
            num_frames=50,  # å‡å°‘å¸§æ•°ç”¨äºæµ‹è¯•
            max_samples=5   # é™åˆ¶æ ·æœ¬æ•°é‡
        )
        
        if len(dataset) == 0:
            print("âŒ æ•°æ®é›†ä¸ºç©º")
            return False
        
        # æµ‹è¯•åŠ è½½ä¸€ä¸ªæ ·æœ¬
        sample = dataset[0]
        pose_sequence = sample['pose_sequence']
        caption = sample['caption']
        
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ")
        print(f"   æ ·æœ¬æ•°é‡: {len(dataset)}")
        print(f"   å§¿æ€å½¢çŠ¶: {pose_sequence.shape}")
        print(f"   ç¤ºä¾‹æ–‡æœ¬: {caption}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return False

def test_model_creation():
    """æµ‹è¯•æ¨¡å‹åˆ›å»º"""
    print("\nğŸ—ï¸ æµ‹è¯•æ¨¡å‹åˆ›å»º...")
    
    try:
        from text2video_model import PoseUNet1D, TextToPoseDiffusion
        
        # åˆ›å»ºæ¨¡å‹
        model = PoseUNet1D(
            pose_dim=120,
            model_channels=64,  # å‡å°‘é€šé“æ•°ç”¨äºæµ‹è¯•
            num_frames=50
        )
        
        # åˆ›å»ºæ‰©æ•£è¿‡ç¨‹
        diffusion = TextToPoseDiffusion(
            num_timesteps=100,  # å‡å°‘æ­¥æ•°ç”¨äºæµ‹è¯•
            beta_schedule='cosine'
        )
        
        print(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        print(f"   æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        print(f"   æ‰©æ•£æ­¥æ•°: {diffusion.num_timesteps}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        return False

def test_forward_pass():
    """æµ‹è¯•å‰å‘ä¼ æ’­"""
    print("\nâš¡ æµ‹è¯•å‰å‘ä¼ æ’­...")
    
    try:
        from text2video_model import PoseUNet1D, TextToPoseDiffusion
        
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"   ä½¿ç”¨è®¾å¤‡: {device}")
        
        # åˆ›å»ºæ¨¡å‹
        model = PoseUNet1D(
            pose_dim=120,
            model_channels=64,
            num_frames=50
        ).to(device)
        
        # åˆ›å»ºæ‰©æ•£è¿‡ç¨‹
        diffusion = TextToPoseDiffusion(num_timesteps=100)
        diffusion.betas = diffusion.betas.to(device)
        diffusion.alphas = diffusion.alphas.to(device)
        diffusion.alphas_cumprod = diffusion.alphas_cumprod.to(device)
        diffusion.alphas_cumprod_prev = diffusion.alphas_cumprod_prev.to(device)
        diffusion.posterior_variance = diffusion.posterior_variance.to(device)
        
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        batch_size = 2
        pose_sequence = torch.randn(batch_size, 50, 120).to(device)
        timesteps = torch.randint(0, 100, (batch_size,)).to(device)
        text_prompts = ["hello", "thank you"]
        
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            output = model(pose_sequence, timesteps, text_prompts)
        
        print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸ")
        print(f"   è¾“å…¥å½¢çŠ¶: {pose_sequence.shape}")
        print(f"   è¾“å‡ºå½¢çŠ¶: {output.shape}")
        
        # æµ‹è¯•æŸå¤±è®¡ç®—
        loss = diffusion.p_losses(model, pose_sequence, timesteps, text_prompts)
        print(f"   æŸå¤±è®¡ç®—æˆåŠŸï¼ŒæŸå¤±å€¼: {loss.item():.6f}")
        
        return True
        
    except Exception as e:
        print(f"âŒ å‰å‘ä¼ æ’­å¤±è´¥: {e}")
        return False

def test_training_step():
    """æµ‹è¯•è®­ç»ƒæ­¥éª¤"""
    print("\nğŸ‹ï¸ æµ‹è¯•è®­ç»ƒæ­¥éª¤...")
    
    try:
        from train_text2video import ASLTextToPoseTrainer
        from text2video_model import PoseUNet1D, TextToPoseDiffusion
        
        # æŸ¥æ‰¾æ•°æ®ç›®å½•
        data_dir = check_asl_data()
        if not data_dir:
            return False
        
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åˆ›å»ºæ¨¡å‹
        model = PoseUNet1D(
            pose_dim=120,
            model_channels=32,  # æ›´å°çš„æ¨¡å‹ç”¨äºæµ‹è¯•
            num_frames=20       # æ›´çŸ­çš„åºåˆ—
        )
        
        diffusion = TextToPoseDiffusion(num_timesteps=50)
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = ASLTextToPoseTrainer(
            model=model,
            diffusion=diffusion,
            data_dir=data_dir,
            device=device,
            batch_size=2,
            learning_rate=1e-3,
            num_epochs=2,
            num_frames=20,
            use_wandb=False
        )
        
        # é™åˆ¶æ•°æ®é›†å¤§å°
        trainer.dataset.data_paths = trainer.dataset.data_paths[:2]
        trainer.dataset.captions = trainer.dataset.captions[:2]
        
        # æµ‹è¯•ä¸€ä¸ªè®­ç»ƒæ­¥éª¤
        for batch in trainer.dataloader:
            loss = trainer.train_step(batch)
            print(f"âœ… è®­ç»ƒæ­¥éª¤æˆåŠŸï¼ŒæŸå¤±: {loss:.6f}")
            break
        
        return True
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒæ­¥éª¤å¤±è´¥: {e}")
        return False

def test_generation():
    """æµ‹è¯•ç”Ÿæˆ"""
    print("\nğŸ¨ æµ‹è¯•å§¿æ€ç”Ÿæˆ...")
    
    try:
        from text2video_model import PoseUNet1D, TextToPoseDiffusion
        
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åˆ›å»ºæ¨¡å‹
        model = PoseUNet1D(
            pose_dim=120,
            model_channels=32,
            num_frames=20
        ).to(device)
        
        diffusion = TextToPoseDiffusion(num_timesteps=20)  # å¾ˆå°‘çš„æ­¥æ•°ç”¨äºå¿«é€Ÿæµ‹è¯•
        diffusion.betas = diffusion.betas.to(device)
        diffusion.alphas = diffusion.alphas.to(device)
        diffusion.alphas_cumprod = diffusion.alphas_cumprod.to(device)
        diffusion.alphas_cumprod_prev = diffusion.alphas_cumprod_prev.to(device)
        diffusion.posterior_variance = diffusion.posterior_variance.to(device)
        
        # ç”Ÿæˆå§¿æ€åºåˆ—
        with torch.no_grad():
            pose_sequences = diffusion.sample(
                model,
                text_prompts=["hello"],
                num_frames=20,
                pose_dim=120
            )
        
        final_pose = pose_sequences[-1][0].cpu().numpy()  # (T, 120)
        
        print(f"âœ… å§¿æ€ç”ŸæˆæˆåŠŸ")
        print(f"   ç”Ÿæˆå½¢çŠ¶: {final_pose.shape}")
        print(f"   æ•°å€¼èŒƒå›´: [{final_pose.min():.3f}, {final_pose.max():.3f}]")
        
        return True
        
    except Exception as e:
        print(f"âŒ å§¿æ€ç”Ÿæˆå¤±è´¥: {e}")
        return False

def test_pose_visualization():
    """æµ‹è¯•å§¿æ€å¯è§†åŒ–"""
    print("\nğŸ“ˆ æµ‹è¯•å§¿æ€å¯è§†åŒ–...")
    
    try:
        import matplotlib.pyplot as plt
        import numpy as np
        
        # åˆ›å»ºæµ‹è¯•å§¿æ€æ•°æ®
        test_pose = np.random.randn(50, 120) * 10  # (T, 120)
        
        from generate_text2video import ASLTextToPoseGenerator
        
        # åˆ›å»ºä¸€ä¸ªä¸´æ—¶ç”Ÿæˆå™¨å®ä¾‹ï¼ˆä¸éœ€è¦åŠ è½½çœŸå®æ¨¡å‹ï¼‰
        class MockGenerator:
            def visualize_pose_sequence(self, pose_array, text_prompt, save_path=None):
                # ç®€åŒ–çš„å¯è§†åŒ–æµ‹è¯•
                fig, ax = plt.subplots(1, 1, figsize=(8, 6))
                ax.imshow(pose_array.T, aspect='auto', cmap='viridis')
                ax.set_title(f"æµ‹è¯•å§¿æ€: {text_prompt}")
                ax.set_xlabel('å¸§æ•°')
                ax.set_ylabel('ç‰¹å¾ç»´åº¦')
                
                if save_path:
                    plt.savefig(save_path)
                    print(f"   å¯è§†åŒ–ä¿å­˜åˆ°: {save_path}")
                
                plt.close()
                return True
        
        generator = MockGenerator()
        test_output = "./test_visualization.png"
        generator.visualize_pose_sequence(test_pose, "test pose", test_output)
        
        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        if os.path.exists(test_output):
            os.remove(test_output)
        
        print(f"âœ… å§¿æ€å¯è§†åŒ–æˆåŠŸ")
        return True
        
    except Exception as e:
        print(f"âŒ å§¿æ€å¯è§†åŒ–å¤±è´¥: {e}")
        return False

def create_minimal_training_example():
    """åˆ›å»ºæœ€å°è®­ç»ƒç¤ºä¾‹"""
    print("\nğŸ“‹ åˆ›å»ºæœ€å°è®­ç»ƒç¤ºä¾‹...")
    
    # æŸ¥æ‰¾æ•°æ®ç›®å½•
    data_dir = check_asl_data()
    if not data_dir:
        print("âŒ éœ€è¦ASLæ•°æ®æ‰èƒ½åˆ›å»ºè®­ç»ƒç¤ºä¾‹")
        return False
    
    script_content = f'''#!/usr/bin/env python3

# ASL Text-to-Poseæœ€å°è®­ç»ƒç¤ºä¾‹
import torch
from text2video_model import PoseUNet1D, TextToPoseDiffusion
from train_text2video import ASLTextToPoseTrainer

def main():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ä½¿ç”¨è®¾å¤‡: {{device}}")
    
    # åˆ›å»ºå°æ¨¡å‹ç”¨äºå¿«é€Ÿæµ‹è¯•
    model = PoseUNet1D(
        pose_dim=120,
        model_channels=32,
        num_frames=30
    )
    
    diffusion = TextToPoseDiffusion(num_timesteps=100)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = ASLTextToPoseTrainer(
        model=model,
        diffusion=diffusion,
        data_dir="{data_dir}",
        device=device,
        batch_size=2,
        learning_rate=1e-3,
        num_epochs=5,
        num_frames=30,
        use_wandb=False  # å…³é—­wandbç”¨äºæµ‹è¯•
    )
    
    # é™åˆ¶æ•°æ®é›†å¤§å°ç”¨äºå¿«é€Ÿæµ‹è¯•
    trainer.dataset.data_paths = trainer.dataset.data_paths[:5]
    trainer.dataset.captions = trainer.dataset.captions[:5]
    
    print(f"å¼€å§‹è®­ç»ƒ (æ•°æ®é›†å¤§å°: {{len(trainer.dataset)}})")
    trainer.train()

if __name__ == "__main__":
    main()
'''
    
    with open("minimal_train_example.py", "w", encoding="utf-8") as f:
        f.write(script_content)
    
    print("âœ… æœ€å°è®­ç»ƒç¤ºä¾‹å·²åˆ›å»º: minimal_train_example.py")
    print("   è¿è¡Œå‘½ä»¤: python minimal_train_example.py")
    return True

def run_full_test():
    """è¿è¡Œå®Œæ•´æµ‹è¯•æµç¨‹"""
    print("ğŸš€ å¼€å§‹ASL Text-to-Poseé¡¹ç›®å®Œæ•´æµ‹è¯•\n")
    
    tests = [
        ("ä¾èµ–æ£€æŸ¥", check_dependencies),
        ("ASLæ•°æ®æ£€æŸ¥", lambda: check_asl_data() is not None),
        ("æ•°æ®åŠ è½½æµ‹è¯•", test_data_loading),
        ("æ¨¡å‹åˆ›å»ºæµ‹è¯•", test_model_creation),
        ("å‰å‘ä¼ æ’­æµ‹è¯•", test_forward_pass),
        ("è®­ç»ƒæ­¥éª¤æµ‹è¯•", test_training_step),
        ("å§¿æ€ç”Ÿæˆæµ‹è¯•", test_generation),
        ("å¯è§†åŒ–æµ‹è¯•", test_pose_visualization),
        ("åˆ›å»ºè®­ç»ƒç¤ºä¾‹", create_minimal_training_example)
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        try:
            if test_func():
                passed += 1
            else:
                print(f"âš ï¸ {test_name} è·³è¿‡æˆ–å¤±è´¥")
        except Exception as e:
            print(f"âŒ {test_name} å‡ºç°å¼‚å¸¸: {e}")
    
    print(f"\nğŸ“Š æµ‹è¯•æ€»ç»“:")
    print(f"   é€šè¿‡: {passed}/{total}")
    print(f"   æˆåŠŸç‡: {passed/total*100:.1f}%")
    
    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ASL Text-to-Poseé¡¹ç›®è®¾ç½®æ­£ç¡®ã€‚")
        print("\nğŸ“ ä¸‹ä¸€æ­¥:")
        print("   1. è¿è¡Œ python minimal_train_example.py å¼€å§‹è®­ç»ƒ")
        print("   2. ä½¿ç”¨å®Œæ•´æ•°æ®é›†è¿›è¡Œæ­£å¼è®­ç»ƒ")
        print("   3. è°ƒæ•´è¶…å‚æ•°ä¼˜åŒ–æ•ˆæœ")
    else:
        print(f"\nâš ï¸ æœ‰ {total-passed} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³ç»„ä»¶ã€‚")
    
    return passed == total

if __name__ == "__main__":
    if len(sys.argv) > 1:
        test_name = sys.argv[1].lower()
        
        test_map = {
            "deps": check_dependencies,
            "data": test_data_loading,
            "model": test_model_creation,
            "forward": test_forward_pass,
            "train": test_training_step,
            "generate": test_generation,
            "visualize": test_pose_visualization
        }
        
        if test_name in test_map:
            print(f"è¿è¡Œå•é¡¹æµ‹è¯•: {test_name}")
            success = test_map[test_name]()
            sys.exit(0 if success else 1)
        else:
            print(f"æœªçŸ¥æµ‹è¯•: {test_name}")
            print(f"å¯ç”¨æµ‹è¯•: {list(test_map.keys())}")
            sys.exit(1)
    else:
        # è¿è¡Œå®Œæ•´æµ‹è¯•
        success = run_full_test()
        sys.exit(0 if success else 1) 