import torch
import torch.nn.functional as F
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau
import numpy as np
import os
import argparse
from tqdm import tqdm
import wandb
from text2video_model import PoseUNet1D, TextToPoseDiffusion
from video_dataset import create_asl_dataloader
import matplotlib.pyplot as plt
from datetime import datetime
import glob

class ASLTextToPoseTrainer:
    """ASL Text-to-Pose Diffusion Modelè®­ç»ƒå™¨"""
    
    def __init__(
        self,
        model: PoseUNet1D,
        diffusion: TextToPoseDiffusion,
        data_dir: str,
        device: str = "cuda",
        batch_size: int = 8,
        learning_rate: float = 1e-4,
        num_epochs: int = 1000,
        save_interval: int = 5,
        log_interval: int = 10,
        sample_interval: int = 200,
        use_wandb: bool = True,
        project_name: str = "asl-text2pose-diffusion",
        num_frames: int = 100,
        pose_dim: int = 120,
        max_samples: int = None,
        resume_from: str = None,
        lr_scheduler_type: str = "plateau"  # "plateau" æˆ– "cosine"
    ):
        self.model = model.to(device)
        self.diffusion = diffusion
        self.device = device
        self.num_epochs = num_epochs
        self.save_interval = save_interval
        self.log_interval = log_interval
        self.sample_interval = sample_interval
        self.use_wandb = use_wandb
        self.project_name = project_name
        self.num_frames = num_frames
        self.pose_dim = pose_dim
        self.lr_scheduler_type = lr_scheduler_type
        
        # è®­ç»ƒçŠ¶æ€è·Ÿè¸ª
        self.start_epoch = 0
        self.global_step = 0
        self.train_losses = []
        self.best_loss = float('inf')
        self.lr_patience_counter = 0
        
        # ç¡®ä¿æ‰©æ•£è¿‡ç¨‹çš„å¼ é‡åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        self.diffusion.betas = self.diffusion.betas.to(device)
        self.diffusion.alphas = self.diffusion.alphas.to(device)
        self.diffusion.alphas_cumprod = self.diffusion.alphas_cumprod.to(device)
        self.diffusion.alphas_cumprod_prev = self.diffusion.alphas_cumprod_prev.to(device)
        self.diffusion.posterior_variance = self.diffusion.posterior_variance.to(device)
        
        # åˆ›å»ºæ•°æ®é›†å’ŒåŠ è½½å™¨
        print("ğŸ“Š åˆ›å»ºæ•°æ®é›†...")
        self.dataloader, self.dataset = create_asl_dataloader(
            data_dir=data_dir,
            batch_size=batch_size,
            shuffle=True,
            num_workers=0 if os.name == 'nt' else 2,  # Windowsä½¿ç”¨0
            num_frames=num_frames,
            max_samples=max_samples,
            normalize=True,
            augment=False
        )
        
        # ä¼˜åŒ–å™¨
        self.optimizer = AdamW(
            self.model.parameters(),
            lr=learning_rate,
            weight_decay=1e-6
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        if lr_scheduler_type == "plateau":
            # åŸºäºéªŒè¯æŸå¤±çš„åŠ¨æ€è°ƒæ•´
            self.scheduler = ReduceLROnPlateau(
                self.optimizer,
                mode='min',
                factor=0.5,
                patience=10,
                verbose=True,
                threshold=1e-4,
                min_lr=learning_rate * 0.001
            )
        else:
            # åŸºäºepochçš„ä½™å¼¦é€€ç«
            self.scheduler = CosineAnnealingLR(
                self.optimizer,
                T_max=num_epochs,
                eta_min=learning_rate * 0.01
            )
        
        # åˆ›å»ºä¿å­˜ç›®å½•ï¼ˆå¦‚æœæ˜¯resumeï¼Œä½¿ç”¨åŸç›®å½•ï¼‰
        if resume_from:
            self.save_dir = os.path.dirname(resume_from)
            print(f"ğŸ”„ ç»§ç»­è®­ç»ƒæ¨¡å¼ï¼Œä½¿ç”¨ç›®å½•: {self.save_dir}")
        else:
            self.save_dir = f"checkpoints/asl_text2pose_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            os.makedirs(self.save_dir, exist_ok=True)
            print(f"ğŸ†• æ–°è®­ç»ƒæ¨¡å¼ï¼Œåˆ›å»ºç›®å½•: {self.save_dir}")
        
        # è½½å…¥æ£€æŸ¥ç‚¹ï¼ˆå¦‚æœæä¾›ï¼‰
        if resume_from and os.path.exists(resume_from):
            self.load_checkpoint(resume_from)
        elif resume_from:
            print(f"âš ï¸  æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {resume_from}")
            print("ğŸ†• ä»å¤´å¼€å§‹è®­ç»ƒ...")
        
        # Wandbåˆå§‹åŒ–
        if use_wandb:
            wandb_config = {
                "model_channels": model.model_channels,
                "num_timesteps": diffusion.num_timesteps,
                "batch_size": batch_size,
                "learning_rate": learning_rate,
                "num_epochs": num_epochs,
                "num_frames": num_frames,
                "pose_dim": pose_dim,
                "dataset_size": len(self.dataset),
                "lr_scheduler": lr_scheduler_type,
                "resume_from": resume_from is not None
            }
            
            if resume_from:
                # Resume existing run
                wandb.init(
                    project=project_name,
                    config=wandb_config,
                    resume="allow"
                )
            else:
                # New run
                wandb.init(
                    project=project_name,
                    config=wandb_config
                )
        
        print(f"ASL Text-to-Poseè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ:")
        print(f"  è®¾å¤‡: {device}")
        print(f"  æ•°æ®é›†å¤§å°: {len(self.dataset)}")
        print(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"  åˆå§‹å­¦ä¹ ç‡: {learning_rate}")
        print(f"  å­¦ä¹ ç‡è°ƒåº¦: {lr_scheduler_type}")
        print(f"  è®­ç»ƒè½®æ•°: {num_epochs}")
        print(f"  å¼€å§‹epoch: {self.start_epoch}")
        print(f"  å§¿æ€è§„æ ¼: {num_frames}å¸§ x {pose_dim}ç»´")
        print(f"  ä¿å­˜ç›®å½•: {self.save_dir}")
        
    def train_step(self, batch):
        """æ‰§è¡Œä¸€ä¸ªè®­ç»ƒæ­¥éª¤"""
        pose_sequences = batch['pose_sequences'].to(self.device)
        captions = batch['captions']
        
        # éšæœºé‡‡æ ·æ—¶é—´æ­¥
        t = torch.randint(
            0, self.diffusion.num_timesteps, 
            (pose_sequences.shape[0],), 
            device=self.device
        )
        
        # è®¡ç®—æŸå¤±
        loss = self.diffusion.p_losses(self.model, pose_sequences, t, captions)
        
        return loss
    
    def validate(self, num_samples=50):
        """éªŒè¯æ¨¡å‹æ€§èƒ½"""
        self.model.eval()
        total_loss = 0
        num_batches = 0
        
        with torch.no_grad():
            for i, batch in enumerate(self.dataloader):
                if i * self.dataloader.batch_size >= num_samples:
                    break
                    
                pose_sequences = batch['pose_sequences'].to(self.device)
                captions = batch['captions']
                
                t = torch.randint(
                    0, self.diffusion.num_timesteps,
                    (pose_sequences.shape[0],), device=self.device
                ).long()
                
                loss = self.diffusion.p_losses(self.model, pose_sequences, t, captions)
                total_loss += loss.item()
                num_batches += 1
        
        self.model.train()
        return total_loss / num_batches if num_batches > 0 else 0
    
    def sample_poses(self, text_prompts):
        """ç”Ÿæˆå§¿æ€åºåˆ—æ ·æœ¬"""
        self.model.eval()
        
        with torch.no_grad():
            # ç”Ÿæˆæ ·æœ¬
            samples = self.diffusion.sample(
                self.model, 
                text_prompts=text_prompts,
                num_frames=self.num_frames,
                pose_dim=self.pose_dim
            )
            
            # ä½¿ç”¨æœ€åä¸€æ­¥çš„ç»“æœ
            final_samples = samples[-1]  # (batch_size, T, 120)
            
            # åæ ‡å‡†åŒ–
            if hasattr(self.dataset, 'denormalize_pose'):
                denormalized_samples = []
                for i in range(final_samples.shape[0]):
                    denorm_pose = self.dataset.denormalize_pose(final_samples[i].numpy())
                    denormalized_samples.append(denorm_pose)
                final_samples = np.stack(denormalized_samples)
            else:
                final_samples = final_samples.numpy()
        
        self.model.train()
        return final_samples
    
    def visualize_pose_sequence(self, pose_sequence, title="Pose Sequence"):
        """å¯è§†åŒ–å§¿æ€åºåˆ—"""
        # pose_sequence: (T, 120)
        fig, axes = plt.subplots(2, 2, figsize=(12, 8))
        
        # 1. èº«ä½“å…³é”®ç‚¹è½¨è¿¹
        body_coords = pose_sequence[:, :36].reshape(-1, 18, 2)  # (T, 18, 2)
        for joint_idx in range(18):
            if np.any(np.abs(body_coords[:, joint_idx, :]) > 1e-6):
                axes[0, 0].plot(body_coords[:, joint_idx, 0], label=f'Joint {joint_idx}')
        axes[0, 0].set_title('èº«ä½“å…³é”®ç‚¹ X åæ ‡è½¨è¿¹')
        axes[0, 0].set_xlabel('å¸§æ•°')
        axes[0, 0].set_ylabel('X åæ ‡')
        
        # 2. å·¦æ‰‹å…³é”®ç‚¹è½¨è¿¹
        left_hand_coords = pose_sequence[:, 36:78].reshape(-1, 21, 2)  # (T, 21, 2)
        for joint_idx in range(0, 21, 5):  # åªæ˜¾ç¤ºéƒ¨åˆ†å…³é”®ç‚¹
            if np.any(np.abs(left_hand_coords[:, joint_idx, :]) > 1e-6):
                axes[0, 1].plot(left_hand_coords[:, joint_idx, 0], label=f'Joint {joint_idx}')
        axes[0, 1].set_title('å·¦æ‰‹å…³é”®ç‚¹ X åæ ‡è½¨è¿¹')
        axes[0, 1].set_xlabel('å¸§æ•°')
        axes[0, 1].set_ylabel('X åæ ‡')
        
        # 3. å³æ‰‹å…³é”®ç‚¹è½¨è¿¹  
        right_hand_coords = pose_sequence[:, 78:120].reshape(-1, 21, 2)  # (T, 21, 2)
        for joint_idx in range(0, 21, 5):  # åªæ˜¾ç¤ºéƒ¨åˆ†å…³é”®ç‚¹
            if np.any(np.abs(right_hand_coords[:, joint_idx, :]) > 1e-6):
                axes[1, 0].plot(right_hand_coords[:, joint_idx, 0], label=f'Joint {joint_idx}')
        axes[1, 0].set_title('å³æ‰‹å…³é”®ç‚¹ X åæ ‡è½¨è¿¹')
        axes[1, 0].set_xlabel('å¸§æ•°')
        axes[1, 0].set_ylabel('X åæ ‡')
        
        # 4. ç‰¹å¾çƒ­åŠ›å›¾
        im = axes[1, 1].imshow(pose_sequence.T, aspect='auto', cmap='viridis')
        axes[1, 1].set_title('å§¿æ€ç‰¹å¾çƒ­åŠ›å›¾')
        axes[1, 1].set_xlabel('å¸§æ•°')
        axes[1, 1].set_ylabel('ç‰¹å¾ç»´åº¦')
        plt.colorbar(im, ax=axes[1, 1])
        
        plt.suptitle(title, fontsize=14)
        plt.tight_layout()
        
        return fig
    
    def visualize_samples(self, epoch, sample_prompts=None):
        """å¯è§†åŒ–ç”Ÿæˆçš„æ ·æœ¬"""
        if sample_prompts is None:
            sample_prompts = [
                "hello",
                "thank you", 
                "good morning",
                "how are you"
            ]
        
        # é™åˆ¶æ ·æœ¬æ•°é‡ä»¥é¿å…å†…å­˜é—®é¢˜
        sample_prompts = sample_prompts[:2]
        
        try:
            samples = self.sample_poses(sample_prompts)  # (B, T, 120)
            
            for i, (pose_sequence, prompt) in enumerate(zip(samples, sample_prompts)):
                # å¯è§†åŒ–å§¿æ€åºåˆ—
                fig = self.visualize_pose_sequence(pose_sequence, f"ç”Ÿæˆæ ·æœ¬: {prompt}")
                
                # ä¿å­˜å›¾ç‰‡
                save_path = os.path.join(self.save_dir, f'pose_sample_epoch_{epoch}_{i}.png')
                plt.savefig(save_path, dpi=150, bbox_inches='tight')
                
                if self.use_wandb:
                    wandb.log({f"generated_pose_{i}": wandb.Image(save_path)}, step=epoch)
                
                plt.close()
            
            print(f"ç”Ÿæˆæ ·æœ¬å¯è§†åŒ–å·²ä¿å­˜")
            
        except Exception as e:
            print(f"ç”Ÿæˆæ ·æœ¬æ—¶å‡ºé”™: {e}")
            
    def load_checkpoint(self, checkpoint_path):
        """è½½å…¥æ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒ"""
        print(f"ğŸ“‚ è½½å…¥æ£€æŸ¥ç‚¹: {checkpoint_path}")
        
        try:
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            
            # è½½å…¥æ¨¡å‹æƒé‡
            self.model.load_state_dict(checkpoint['model_state_dict'])
            print("âœ… æ¨¡å‹æƒé‡è½½å…¥æˆåŠŸ")
            
            # è½½å…¥ä¼˜åŒ–å™¨çŠ¶æ€
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            print("âœ… ä¼˜åŒ–å™¨çŠ¶æ€è½½å…¥æˆåŠŸ")
            
            # è½½å…¥è°ƒåº¦å™¨çŠ¶æ€
            if 'scheduler_state_dict' in checkpoint:
                self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
                print("âœ… å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€è½½å…¥æˆåŠŸ")
            
            # è½½å…¥è®­ç»ƒçŠ¶æ€
            self.start_epoch = checkpoint.get('epoch', 0) + 1  # ä»ä¸‹ä¸€ä¸ªepochå¼€å§‹
            self.best_loss = checkpoint.get('best_loss', float('inf'))
            self.global_step = checkpoint.get('global_step', 0)
            
            # è½½å…¥æŸå¤±å†å²ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            if 'train_losses' in checkpoint:
                self.train_losses = checkpoint['train_losses']
                print(f"âœ… è½½å…¥ {len(self.train_losses)} ä¸ªepochçš„æŸå¤±å†å²")
            
            # éªŒè¯æ¨¡å‹é…ç½®åŒ¹é…
            config = checkpoint.get('config', {})
            if config:
                model_channels = config.get('model_channels', self.model.model_channels)
                num_frames = config.get('num_frames', self.num_frames)
                pose_dim = config.get('pose_dim', self.pose_dim)
                
                if (model_channels != self.model.model_channels or 
                    num_frames != self.num_frames or 
                    pose_dim != self.pose_dim):
                    print("âš ï¸  æ¨¡å‹é…ç½®ä¸åŒ¹é…ï¼Œå¯èƒ½å¯¼è‡´é—®é¢˜:")
                    print(f"   æ£€æŸ¥ç‚¹: channels={model_channels}, frames={num_frames}, pose_dim={pose_dim}")
                    print(f"   å½“å‰:   channels={self.model.model_channels}, frames={self.num_frames}, pose_dim={self.pose_dim}")
            
            print(f"ğŸ¯ ç»§ç»­è®­ç»ƒçŠ¶æ€:")
            print(f"   å¼€å§‹epoch: {self.start_epoch}")
            print(f"   å…¨å±€æ­¥æ•°: {self.global_step}")
            print(f"   å½“å‰æœ€ä½³æŸå¤±: {self.best_loss:.6f}")
            print(f"   å½“å‰å­¦ä¹ ç‡: {self.optimizer.param_groups[0]['lr']:.2e}")
            
        except Exception as e:
            print(f"âŒ è½½å…¥æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            print("ğŸ†• å°†ä»å¤´å¼€å§‹è®­ç»ƒ...")
            self.start_epoch = 0
            self.global_step = 0
            self.best_loss = float('inf')
            
    def save_checkpoint(self, epoch, loss):
        """ä¿å­˜æ£€æŸ¥ç‚¹ - ç©ºé—´èŠ‚çœç­–ç•¥ï¼šåªä¿ç•™3ä¸ªå›ºå®šæ–‡ä»¶"""
        checkpoint = {
            'epoch': epoch,
            'global_step': self.global_step,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'loss': loss,
            'best_loss': self.best_loss,
            'train_losses': self.train_losses,  # ä¿å­˜æŸå¤±å†å²
            'lr_scheduler_type': self.lr_scheduler_type,
            'config': {
                'num_frames': self.num_frames,
                'pose_dim': self.pose_dim,
                'model_channels': self.model.model_channels,
                'num_timesteps': self.diffusion.num_timesteps,
            },
            'dataset_stats': {
                'pose_mean': self.dataset.pose_mean if hasattr(self.dataset, 'pose_mean') else None,
                'pose_std': self.dataset.pose_std if hasattr(self.dataset, 'pose_std') else None,
            }
        }
        
        # 1. å§‹ç»ˆä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹ï¼ˆè¦†ç›–ï¼‰
        latest_path = os.path.join(self.save_dir, 'latest.pth')
        torch.save(checkpoint, latest_path)
        
        # 2. å¦‚æœæ˜¯æœ€ä½³æ¨¡å‹ï¼Œä¿å­˜æœ€ä½³æ£€æŸ¥ç‚¹ï¼ˆè¦†ç›–ï¼‰
        if loss < self.best_loss:
            self.best_loss = loss
            best_path = os.path.join(self.save_dir, 'best.pth')
            torch.save(checkpoint, best_path)
            print(f"ğŸ† ä¿å­˜æœ€ä½³æ¨¡å‹ (æŸå¤±: {loss:.6f} -> {self.best_loss:.6f})")
        
        # 3. æ¯50ä¸ªepochä¿å­˜ä¸€æ¬¡å¤‡ä»½ï¼ˆè¦†ç›–ï¼‰
        if epoch % 50 == 0 and epoch > 0:
            backup_path = os.path.join(self.save_dir, 'backup.pth')
            torch.save(checkpoint, backup_path)
            print(f"ğŸ’¾ ä¿å­˜å¤‡ä»½æ£€æŸ¥ç‚¹ (Epoch {epoch})")
        
        # æ˜¾ç¤ºæ–‡ä»¶å¤§å°ä¿¡æ¯
        try:
            latest_size = os.path.getsize(latest_path) / (1024*1024)  # MB
            best_path = os.path.join(self.save_dir, 'best.pth')
            backup_path = os.path.join(self.save_dir, 'backup.pth')
            
            print(f"ğŸ’¿ æ£€æŸ¥ç‚¹æ–‡ä»¶çŠ¶æ€:")
            print(f"   latest.pth: {latest_size:.1f}MB (Epoch {epoch})")
            
            if os.path.exists(best_path):
                best_size = os.path.getsize(best_path) / (1024*1024)
                print(f"   best.pth: {best_size:.1f}MB (æŸå¤±: {self.best_loss:.6f})")
            
            if os.path.exists(backup_path):
                backup_size = os.path.getsize(backup_path) / (1024*1024)
                backup_epoch = torch.load(backup_path, map_location='cpu')['epoch']
                print(f"   backup.pth: {backup_size:.1f}MB (Epoch {backup_epoch})")
                
        except Exception as e:
            print(f"âš ï¸  æ— æ³•è·å–æ–‡ä»¶å¤§å°ä¿¡æ¯: {e}")
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print("å¼€å§‹è®­ç»ƒASL Text-to-Poseæ¨¡å‹...")
        
        self.model.train()
        
        # å¦‚æœæ˜¯ç»§ç»­è®­ç»ƒï¼Œæ˜¾ç¤ºæ¢å¤ä¿¡æ¯
        if self.start_epoch > 0:
            print(f"ğŸ”„ ä»ç¬¬ {self.start_epoch+1} ä¸ªepochç»§ç»­è®­ç»ƒ")
            print(f"   å½“å‰æœ€ä½³æŸå¤±: {self.best_loss:.6f}")
            print(f"   å…¨å±€æ­¥æ•°: {self.global_step}")
        
        for epoch in range(self.start_epoch, self.num_epochs):
            epoch_losses = []
            
            # è®­ç»ƒä¸€ä¸ªepoch
            pbar = tqdm(self.dataloader, desc=f'Epoch {epoch+1}/{self.num_epochs}')
            
            for batch in pbar:
                loss = self.train_step(batch)
                
                # åå‘ä¼ æ’­
                self.optimizer.zero_grad()
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                
                self.optimizer.step()
                
                # å°†æŸå¤±è½¬æ¢ä¸ºCPUæ ‡é‡
                loss_value = loss.item()
                epoch_losses.append(loss_value)
                self.global_step += 1
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_postfix({
                    'loss': f'{loss_value:.6f}',
                    'lr': f'{self.optimizer.param_groups[0]["lr"]:.2e}',
                    'best': f'{self.best_loss:.6f}'
                })
                
                # è®°å½•æ—¥å¿—
                if self.global_step % self.log_interval == 0 and self.use_wandb:
                    wandb.log({
                        'train_loss': loss_value,
                        'learning_rate': self.optimizer.param_groups[0]['lr'],
                        'epoch': epoch,
                        'best_loss': self.best_loss
                    }, step=self.global_step)
            
            # è®¡ç®—epochå¹³å‡æŸå¤±
            avg_loss = np.mean(epoch_losses)
            self.train_losses.append(avg_loss)
            
            # éªŒè¯å’Œå­¦ä¹ ç‡è°ƒæ•´
            val_loss = None
            if epoch % 10 == 0:  # æ¯10ä¸ªepochéªŒè¯ä¸€æ¬¡
                val_loss = self.validate()
                print(f"Epoch {epoch+1}: Train Loss = {avg_loss:.6f}, Val Loss = {val_loss:.6f}")
                
                # æ ¹æ®è°ƒåº¦å™¨ç±»å‹è°ƒæ•´å­¦ä¹ ç‡
                if self.lr_scheduler_type == "plateau":
                    old_lr = self.optimizer.param_groups[0]['lr']
                    self.scheduler.step(val_loss)  # åŸºäºéªŒè¯æŸå¤±
                    new_lr = self.optimizer.param_groups[0]['lr']
                    
                    if new_lr < old_lr:
                        print(f"ğŸ“‰ å­¦ä¹ ç‡é™ä½: {old_lr:.2e} -> {new_lr:.2e}")
                        self.lr_patience_counter = 0
                    else:
                        self.lr_patience_counter += 1
                        
                else:
                    self.scheduler.step()  # åŸºäºepoch
                
                if self.use_wandb:
                    wandb.log({
                        'epoch': epoch,
                        'avg_train_loss': avg_loss,
                        'val_loss': val_loss,
                        'learning_rate': self.optimizer.param_groups[0]['lr'],
                        'lr_patience': self.lr_patience_counter
                    }, step=self.global_step)
            else:
                # ééªŒè¯epochï¼Œä»éœ€è°ƒæ•´å­¦ä¹ ç‡ï¼ˆå¦‚æœæ˜¯cosineè°ƒåº¦ï¼‰
                if self.lr_scheduler_type == "cosine":
                    self.scheduler.step()
            
            # æ—©åœæ£€æŸ¥ï¼ˆå¦‚æœå­¦ä¹ ç‡è¿‡å°ä¸”å¾ˆä¹…æ²¡æœ‰æ”¹å–„ï¼‰
            current_lr = self.optimizer.param_groups[0]['lr']
            if (self.lr_scheduler_type == "plateau" and 
                current_lr < 1e-7 and 
                self.lr_patience_counter > 20):
                print(f"ğŸ›‘ å­¦ä¹ ç‡è¿‡å° ({current_lr:.2e}) ä¸”é•¿æ—¶é—´æ— æ”¹å–„ï¼Œæå‰åœæ­¢è®­ç»ƒ")
                break
            
            # ç”Ÿæˆæ ·æœ¬
            if epoch % self.sample_interval == 0:
                print(f"ğŸ¨ ç”Ÿæˆå§¿æ€æ ·æœ¬ (Epoch {epoch+1})")
                self.visualize_samples(epoch)
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if epoch % self.save_interval == 0 or epoch == self.num_epochs - 1:
                self.save_checkpoint(epoch, avg_loss)
                
            # ç®€å•æ‰“å°ï¼ˆééªŒè¯epochï¼‰
            if epoch % 10 != 0:
                print(f"Epoch {epoch+1}: Train Loss = {avg_loss:.6f}, LR = {current_lr:.2e}")
        
        print("è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ† æœ€ä½³æŸå¤±: {self.best_loss:.6f}")
        print(f"ğŸ“Š æ€»è®­ç»ƒæ­¥æ•°: {self.global_step}")
        
        if self.use_wandb:
            wandb.finish()

def main():
    parser = argparse.ArgumentParser(description='è®­ç»ƒASL Text-to-Pose Diffusion Model')
    parser.add_argument('--data_dir', type=str, default='datasets/signllm_training_data/ASL./dev', help='ASLæ•°æ®ç›®å½•è·¯å¾„')
    parser.add_argument('--batch_size', type=int, default=8, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--learning_rate', type=float, default=1e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--num_epochs', type=int, default=1000, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--model_channels', type=int, default=256, help='æ¨¡å‹é€šé“æ•°')
    parser.add_argument('--num_timesteps', type=int, default=1000, help='æ‰©æ•£æ­¥æ•°')
    parser.add_argument('--num_frames', type=int, default=100, help='åºåˆ—å¸§æ•°')
    parser.add_argument('--pose_dim', type=int, default=120, help='å§¿æ€ç‰¹å¾ç»´åº¦')
    parser.add_argument('--device', type=str, default='cuda', help='è®¾å¤‡')
    parser.add_argument('--no_wandb', action='store_true', help='ç¦ç”¨wandbæ—¥å¿—è®°å½•')
    parser.add_argument('--max_samples', type=int, default=None, help='æœ€å¤§æ ·æœ¬æ•°é‡ï¼ˆç”¨äºè°ƒè¯•ï¼‰')
    parser.add_argument('--resume_from', type=str, default=None, help='ä»å“ªä¸ªæ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒ')
    parser.add_argument('--lr_scheduler_type', type=str, default="plateau", help='å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹')
    
    args = parser.parse_args()
    
    # ğŸ” è‡ªåŠ¨æ£€æµ‹ç°æœ‰æ£€æŸ¥ç‚¹
    print("ğŸ” æ£€æŸ¥ç°æœ‰è®­ç»ƒæ£€æŸ¥ç‚¹...")
    checkpoint_dirs = glob.glob("checkpoints/asl_text2pose_*")
    
    if args.resume_from is None and checkpoint_dirs:
        # æ‰¾åˆ°æœ€æ–°çš„æ£€æŸ¥ç‚¹ç›®å½•
        latest_checkpoint_dir = max(checkpoint_dirs, key=os.path.getmtime)
        
        # ä¼˜å…ˆçº§ï¼šlatest.pth > best.pth > backup.pth
        possible_checkpoints = [
            os.path.join(latest_checkpoint_dir, 'latest.pth'),
            os.path.join(latest_checkpoint_dir, 'best.pth'),
            os.path.join(latest_checkpoint_dir, 'backup.pth')
        ]
        
        for checkpoint_path in possible_checkpoints:
            if os.path.exists(checkpoint_path):
                args.resume_from = checkpoint_path
                print(f"ğŸ¯ è‡ªåŠ¨æ£€æµ‹åˆ°æ£€æŸ¥ç‚¹: {checkpoint_path}")
                break
    
    if args.resume_from:
        print(f"ğŸ“‚ å°†ä»æ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒ: {args.resume_from}")
    else:
        print("ğŸ†• æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒ")
    
    # æ£€æŸ¥æ•°æ®ç›®å½•
    if not os.path.exists(args.data_dir):
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {args.data_dir}")
        print("ğŸ“ è¯·æ£€æŸ¥æ•°æ®è·¯å¾„æ˜¯å¦æ­£ç¡®")
        return
    
    # æ˜¾ç¤ºè®­ç»ƒé…ç½®
    print(f"\nğŸ“‹ è®­ç»ƒé…ç½®:")
    print(f"   æ•°æ®ç›®å½•: {args.data_dir}")
    print(f"   æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"   å­¦ä¹ ç‡: {args.learning_rate}")
    print(f"   è®­ç»ƒè½®æ•°: {args.num_epochs}")
    print(f"   æ¨¡å‹é€šé“æ•°: {args.model_channels}")
    print(f"   æ‰©æ•£æ­¥æ•°: {args.num_timesteps}")
    print(f"   åºåˆ—é•¿åº¦: {args.num_frames}å¸§")
    print(f"   å§¿æ€ç»´åº¦: {args.pose_dim}ç»´")
    print(f"   å­¦ä¹ ç‡è°ƒåº¦: {args.lr_scheduler_type}")
    print(f"   è®¾å¤‡: {args.device}")
    
    # æ£€æŸ¥è®¾å¤‡
    if args.device == 'cuda' and not torch.cuda.is_available():
        print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ°CPU")
        args.device = 'cpu'
    elif args.device == 'cuda':
        print(f"ğŸš€ ä½¿ç”¨GPU: {torch.cuda.get_device_name()}")
        print(f"   æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    
    # åˆ›å»ºæ¨¡å‹
    print("\nğŸ—ï¸  åˆ›å»ºæ¨¡å‹...")
    model = PoseUNet1D(
        pose_dim=args.pose_dim,
        model_channels=args.model_channels,
        num_frames=args.num_frames
    )
    
    # åˆ›å»ºæ‰©æ•£è¿‡ç¨‹
    diffusion = TextToPoseDiffusion(
        num_timesteps=args.num_timesteps,
        beta_schedule='cosine'
    )
    
    # ç§»åŠ¨diffusionçš„å‚æ•°åˆ°è®¾å¤‡
    diffusion.betas = diffusion.betas.to(args.device)
    diffusion.alphas = diffusion.alphas.to(args.device)
    diffusion.alphas_cumprod = diffusion.alphas_cumprod.to(args.device)
    diffusion.alphas_cumprod_prev = diffusion.alphas_cumprod_prev.to(args.device)
    diffusion.posterior_variance = diffusion.posterior_variance.to(args.device)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = ASLTextToPoseTrainer(
        model=model,
        diffusion=diffusion,
        data_dir=args.data_dir,
        device=args.device,
        batch_size=args.batch_size,
        learning_rate=args.learning_rate,
        num_epochs=args.num_epochs,
        num_frames=args.num_frames,
        pose_dim=args.pose_dim,
        max_samples=args.max_samples,
        resume_from=args.resume_from,
        lr_scheduler_type=args.lr_scheduler_type,
        use_wandb=not args.no_wandb
    )
    
    # å¦‚æœæ˜¯è°ƒè¯•æ¨¡å¼ï¼Œé™åˆ¶æ•°æ®é‡
    if args.max_samples:
        trainer.dataset.data_paths = trainer.dataset.data_paths[:args.max_samples]
        trainer.dataset.captions = trainer.dataset.captions[:args.max_samples]
        print(f"ğŸ› è°ƒè¯•æ¨¡å¼ï¼šé™åˆ¶æ•°æ®é›†å¤§å°ä¸º {len(trainer.dataset)} ä¸ªæ ·æœ¬")
    
    # æ˜¾ç¤ºæ•°æ®é›†ä¿¡æ¯
    print(f"\nğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
    print(f"   æ€»æ ·æœ¬æ•°: {len(trainer.dataset)}")
    print(f"   æ¯æ‰¹æ¬¡: {args.batch_size} ä¸ªæ ·æœ¬")
    print(f"   æ€»æ‰¹æ¬¡æ•°: {len(trainer.dataloader)}")
    print(f"   é¢„è®¡æ¯epochæ—¶é—´: ~{len(trainer.dataloader) * args.batch_size / 100:.1f}åˆ†é’Ÿ")
    
    # å¼€å§‹è®­ç»ƒ
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    try:
        trainer.train()
    except KeyboardInterrupt:
        print("\nâ¹ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        print("ğŸ’¾ æ­£åœ¨ä¿å­˜å½“å‰çŠ¶æ€...")
        # ä¿å­˜ä¸­æ–­æ—¶çš„æ£€æŸ¥ç‚¹
        if hasattr(trainer, 'train_losses') and trainer.train_losses:
            current_epoch = trainer.start_epoch + len(trainer.train_losses) - 1
            current_loss = trainer.train_losses[-1]
            trainer.save_checkpoint(current_epoch, current_loss)
            print("âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜ï¼Œå¯ä»¥ä½¿ç”¨ --resume_from ç»§ç»­è®­ç»ƒ")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    
    print("\nğŸ‰ è®­ç»ƒè„šæœ¬æ‰§è¡Œå®Œæˆï¼")

if __name__ == "__main__":
    main() 