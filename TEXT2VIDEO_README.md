# Text-to-Videoæ‰©æ•£æ¨¡å‹ä½¿ç”¨æŒ‡å—

è¿™ä¸ªæŒ‡å—å°†å¸®åŠ©æ‚¨å°†ç°æœ‰çš„3Då§¿æ€ç”Ÿæˆé¡¹ç›®æ”¹é€ ä¸ºtext-to-videoåŠŸèƒ½ã€‚

## ğŸ”„ ä¸»è¦å˜åŒ–

### ä»3Då§¿æ€ç”Ÿæˆåˆ°Text-to-Videoçš„æ”¹é€ 

| æ–¹é¢ | åŸé¡¹ç›® (3Då§¿æ€) | æ–°é¡¹ç›® (Text-to-Video) |
|------|----------------|----------------------|
| **æ•°æ®æ ¼å¼** | `(67, 3)` å•å¸§å§¿æ€ | `(3, T, H, W)` è§†é¢‘åºåˆ— |
| **æ¨¡å‹æ¶æ„** | 1D U-Net | 3D U-Net + äº¤å‰æ³¨æ„åŠ› |
| **è¾“å…¥æ¡ä»¶** | æ— æ¡ä»¶ç”Ÿæˆ | æ–‡æœ¬æ¡ä»¶åŒ–ç”Ÿæˆ |
| **è¾“å‡º** | 3Då…³é”®ç‚¹åæ ‡ | RGBè§†é¢‘å¸§ |
| **è®­ç»ƒæ•°æ®** | `.skels`æ–‡ä»¶ | è§†é¢‘-æ–‡æœ¬é…å¯¹ |

## ğŸ“¦ å®‰è£…ä¾èµ–

```bash
pip install -r requirements_text2video.txt
```

## ğŸ—‚ï¸ æ•°æ®å‡†å¤‡

### æ•°æ®ç›®å½•ç»“æ„

```
your_video_data/
â”œâ”€â”€ videos/
â”‚   â”œâ”€â”€ video_001.mp4
â”‚   â”œâ”€â”€ video_002.mp4
â”‚   â””â”€â”€ ...
â””â”€â”€ captions/
    â”œâ”€â”€ video_001.txt
    â”œâ”€â”€ video_002.txt
    â””â”€â”€ ...
```

### åˆ›å»ºæ¼”ç¤ºæ•°æ®é›†

```bash
python video_dataset.py
```

è¿™å°†åˆ›å»ºä¸€ä¸ªåŒ…å«ç®€å•ç§»åŠ¨æ–¹å—çš„æ¼”ç¤ºæ•°æ®é›†ï¼Œç”¨äºæµ‹è¯•ç³»ç»Ÿã€‚

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### 1. è®­ç»ƒæ¨¡å‹

```bash
# åŸºç¡€è®­ç»ƒ
python train_text2video.py --data_dir ./your_video_data --batch_size 2 --num_epochs 1000

# è°ƒè¯•æ¨¡å¼ï¼ˆå°æ•°æ®é›†ï¼‰
python train_text2video.py --data_dir ./demo_video_data --batch_size 1 --num_epochs 100 --max_samples 5 --no_wandb

# è‡ªå®šä¹‰å‚æ•°
python train_text2video.py \
    --data_dir ./your_video_data \
    --batch_size 4 \
    --learning_rate 1e-4 \
    --num_epochs 2000 \
    --num_frames 16 \
    --frame_size 64 \
    --model_channels 128
```

### 2. ç”Ÿæˆè§†é¢‘

#### å‘½ä»¤è¡Œæ¨¡å¼

```bash
# å•ä¸ªè§†é¢‘ç”Ÿæˆ
python generate_text2video.py \
    --checkpoint ./checkpoints/text2video_xxx/best_model.pth \
    --prompts "A red car driving on a highway" \
    --output_dir ./generated_videos \
    --save_gif --visualize

# æ‰¹é‡ç”Ÿæˆ
python generate_text2video.py \
    --checkpoint ./checkpoints/text2video_xxx/best_model.pth \
    --prompts "A cat playing with a ball" "A person walking in a park" "Ocean waves at sunset" \
    --output_dir ./generated_videos \
    --save_frames --save_gif --visualize
```

#### äº¤äº’å¼æ¨¡å¼

```bash
python generate_text2video.py
```

ç„¶åæŒ‰æç¤ºè¾“å…¥æ¨¡å‹è·¯å¾„å’Œæ–‡æœ¬æè¿°ã€‚

## ğŸ—ï¸ æ¨¡å‹æ¶æ„è¯¦è§£

### æ ¸å¿ƒç»„ä»¶

1. **æ–‡æœ¬ç¼–ç å™¨ (TextEncoder)**
   - åŸºäºCLIPæ¨¡å‹
   - å°†æ–‡æœ¬è½¬æ¢ä¸º768ç»´å‘é‡åºåˆ—
   - æ”¯æŒæœ€é•¿77ä¸ªtoken

2. **3D U-Net (UNet3D)**
   - å¤„ç†5Då¼ é‡ `(B, C, T, H, W)`
   - åŒ…å«æ—¶é—´æ³¨æ„åŠ›æœºåˆ¶
   - äº¤å‰æ³¨æ„åŠ›èåˆæ–‡æœ¬æ¡ä»¶

3. **æ‰©æ•£è¿‡ç¨‹ (TextToVideoDiffusion)**
   - æ‰©å±•åˆ°è§†é¢‘ç»´åº¦çš„DDPM
   - æ”¯æŒæ–‡æœ¬æ¡ä»¶åŒ–ç”Ÿæˆ
   - ä½™å¼¦å™ªå£°è°ƒåº¦

### å…³é”®ç‰¹æ€§

- **äº¤å‰æ³¨æ„åŠ›**: åœ¨æ¯ä¸ªæ®‹å·®å—ä¸­èåˆæ–‡æœ¬ä¿¡æ¯
- **æ—¶é—´å»ºæ¨¡**: æ²¿æ—¶é—´è½´çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶
- **3Då·ç§¯**: åŒæ—¶å¤„ç†ç©ºé—´å’Œæ—¶é—´ç»´åº¦
- **å¯æ‰©å±•æ€§**: æ”¯æŒä¸åŒçš„è§†é¢‘é•¿åº¦å’Œåˆ†è¾¨ç‡

## âš™ï¸ é…ç½®å‚æ•°

### è®­ç»ƒå‚æ•°

```python
# æ¨¡å‹é…ç½®
model_channels = 128        # åŸºç¡€é€šé“æ•°
num_frames = 16            # è§†é¢‘å¸§æ•°
frame_size = 64            # å¸§åˆ†è¾¨ç‡
batch_size = 4             # æ‰¹æ¬¡å¤§å°

# è®­ç»ƒé…ç½®
learning_rate = 1e-4       # å­¦ä¹ ç‡
num_epochs = 1000          # è®­ç»ƒè½®æ•°
num_timesteps = 1000       # æ‰©æ•£æ­¥æ•°

# æ•°æ®é…ç½®
normalize = True           # æ ‡å‡†åŒ–åˆ°[-1, 1]
augment = True            # æ•°æ®å¢å¼º
```

### ç”Ÿæˆå‚æ•°

```python
num_inference_steps = 1000  # æ¨ç†æ­¥æ•°
fps = 8                    # è¾“å‡ºè§†é¢‘å¸§ç‡
save_gif = True           # ä¿å­˜GIFåŠ¨ç”»
save_frames = True        # ä¿å­˜å•ç‹¬å¸§
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### å†…å­˜ä¼˜åŒ–

1. **é™ä½åˆ†è¾¨ç‡**: ä½¿ç”¨è¾ƒå°çš„`frame_size`
2. **å‡å°‘å¸§æ•°**: ä½¿ç”¨è¾ƒå°‘çš„`num_frames`
3. **å‡å°æ‰¹æ¬¡**: é™ä½`batch_size`
4. **æ¢¯åº¦ç´¯ç§¯**: åœ¨å°æ‰¹æ¬¡ä¸Šç´¯ç§¯æ¢¯åº¦

### è®­ç»ƒæŠ€å·§

1. **é¢„è®­ç»ƒæƒé‡**: ä»2Dæ‰©æ•£æ¨¡å‹åˆå§‹åŒ–
2. **æ¸è¿›è®­ç»ƒ**: å…ˆè®­ç»ƒä½åˆ†è¾¨ç‡ï¼Œå†ç²¾è°ƒé«˜åˆ†è¾¨ç‡
3. **æ··åˆç²¾åº¦**: ä½¿ç”¨`accelerate`åº“åŠ é€Ÿè®­ç»ƒ
4. **å­¦ä¹ ç‡è°ƒåº¦**: ä½¿ç”¨ä½™å¼¦é€€ç«è°ƒåº¦å™¨

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **CUDAå†…å­˜ä¸è¶³**
   ```bash
   # å‡å°æ‰¹æ¬¡å¤§å°
   python train_text2video.py --batch_size 1
   
   # å‡å°æ¨¡å‹å¤§å°
   python train_text2video.py --model_channels 64 --frame_size 32
   ```

2. **æ–‡æœ¬ç¼–ç å™¨é”™è¯¯**
   ```bash
   # ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸ï¼ŒCLIPæ¨¡å‹ä¼šè‡ªåŠ¨ä¸‹è½½
   pip install transformers --upgrade
   ```

3. **è§†é¢‘åŠ è½½å¤±è´¥**
   ```bash
   # æ£€æŸ¥OpenCVå®‰è£…
   pip install opencv-python --upgrade
   
   # æ£€æŸ¥è§†é¢‘æ ¼å¼æ”¯æŒ
   python -c "import cv2; print(cv2.getBuildInformation())"
   ```

### è°ƒè¯•æ¨¡å¼

```bash
# ä½¿ç”¨å°æ•°æ®é›†å¿«é€Ÿæµ‹è¯•
python train_text2video.py \
    --data_dir ./demo_video_data \
    --max_samples 3 \
    --num_epochs 10 \
    --batch_size 1 \
    --no_wandb
```

## ğŸ“ˆ æ€§èƒ½ç›‘æ§

### Wandbé›†æˆ

```python
# å¯ç”¨wandbæ—¥å¿—
python train_text2video.py --data_dir ./data  # é»˜è®¤å¯ç”¨

# ç¦ç”¨wandb
python train_text2video.py --data_dir ./data --no_wandb
```

### ç›‘æ§æŒ‡æ ‡

- è®­ç»ƒæŸå¤± (MSE Loss)
- éªŒè¯æŸå¤±
- å­¦ä¹ ç‡å˜åŒ–
- ç”Ÿæˆæ ·æœ¬è´¨é‡
- è§†é¢‘å¸§åºåˆ—

## ğŸ¨ é«˜çº§åŠŸèƒ½

### è‡ªå®šä¹‰æ•°æ®é›†

```python
# ç»§æ‰¿TextVideoDatasetç±»
class CustomVideoDataset(TextVideoDataset):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
    
    def augment_text(self, text):
        # è‡ªå®šä¹‰æ–‡æœ¬å¢å¼º
        return enhanced_text
    
    def augment_video(self, frames):
        # è‡ªå®šä¹‰è§†é¢‘å¢å¼º
        return enhanced_frames
```

### æ¨¡å‹æ‰©å±•

```python
# æ·»åŠ æ›´å¤šæ¡ä»¶ä¿¡æ¯
class ConditionalUNet3D(UNet3D):
    def __init__(self, additional_condition_dim, **kwargs):
        super().__init__(**kwargs)
        self.condition_proj = nn.Linear(additional_condition_dim, self.model_channels)
```

## ğŸ“š å‚è€ƒèµ„æ–™

### ç›¸å…³è®ºæ–‡

1. **Video Diffusion Models** - Ho et al.
2. **Imagen Video** - Google Research
3. **Make-A-Video** - Meta AI
4. **Text2Video-Zero** - Microsoft Research

### ä»£ç èµ„æº

- [Diffusers Library](https://github.com/huggingface/diffusers)
- [Video Diffusion PyTorch](https://github.com/lucidrains/video-diffusion-pytorch)
- [CLIP](https://github.com/openai/CLIP)

## ğŸš§ æœªæ¥æ”¹è¿›

### çŸ­æœŸç›®æ ‡

1. **æ›´é«˜åˆ†è¾¨ç‡**: æ”¯æŒ256x256æˆ–512x512
2. **æ›´é•¿è§†é¢‘**: æ”¯æŒ32å¸§æˆ–64å¸§
3. **æ›´å¥½çš„æ–‡æœ¬ç†è§£**: ä½¿ç”¨æ›´å¤§çš„è¯­è¨€æ¨¡å‹

### é•¿æœŸç›®æ ‡

1. **è§†é¢‘ç¼–è¾‘**: æ”¯æŒå±€éƒ¨ç¼–è¾‘å’Œé£æ ¼è½¬æ¢
2. **å¤šæ¨¡æ€æ¡ä»¶**: ç»“åˆéŸ³é¢‘ã€æ·±åº¦ç­‰ä¿¡æ¯
3. **å®æ—¶ç”Ÿæˆ**: ä¼˜åŒ–æ¨ç†é€Ÿåº¦

## ğŸ’¡ ä½¿ç”¨å»ºè®®

### æ•°æ®å‡†å¤‡å»ºè®®

1. **è§†é¢‘è´¨é‡**: ä½¿ç”¨é«˜è´¨é‡ã€æ— æ°´å°çš„è§†é¢‘
2. **æ–‡æœ¬æè¿°**: ç¡®ä¿æè¿°å‡†ç¡®ã€è¯¦ç»†
3. **æ•°æ®å¤šæ ·æ€§**: åŒ…å«ä¸åŒåœºæ™¯ã€åŠ¨ä½œã€é£æ ¼
4. **æ•°æ®æ¸…æ´—**: å»é™¤æŸåæˆ–ä¸ç›¸å…³çš„æ•°æ®

### è®­ç»ƒå»ºè®®

1. **ä»å°è§„æ¨¡å¼€å§‹**: å…ˆç”¨å°æ•°æ®é›†éªŒè¯æµç¨‹
2. **ç›‘æ§è¿‡æ‹Ÿåˆ**: å®šæœŸæ£€æŸ¥éªŒè¯æŸå¤±
3. **ä¿å­˜æ£€æŸ¥ç‚¹**: å®šæœŸä¿å­˜æ¨¡å‹ä»¥é˜²æ„å¤–ä¸­æ–­
4. **å®éªŒè®°å½•**: ä½¿ç”¨wandbè®°å½•å®éªŒå‚æ•°å’Œç»“æœ

---

**ç¥æ‚¨æˆåŠŸå®ç°text-to-videoåŠŸèƒ½ï¼å¦‚æœ‰é—®é¢˜ï¼Œè¯·æ£€æŸ¥æ—¥å¿—è¾“å‡ºæˆ–æäº¤issueã€‚** 