#!/usr/bin/env python3
"""
æ¨¡å‹å¤§å°æµ‹è¯•è„šæœ¬ - æ¯”è¾ƒä¸åŒé…ç½®çš„å‚æ•°é‡
"""

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent))

from signllm_model import SignLLM, ModelConfig, CONFIG
import torch

def test_model_sizes():
    """æµ‹è¯•ä¸åŒæ¨¡å‹å¤§å°çš„å‚æ•°é‡"""
    sizes = ["tiny", "small", "medium", "large"]
    
    print("ğŸ” SignLLMæ¨¡å‹å¤§å°æ¯”è¾ƒ")
    print("=" * 60)
    
    for size in sizes:
        print(f"\nğŸ“Š {size.upper()} æ¨¡å‹:")
        
        # æ›´æ–°é…ç½®
        global CONFIG
        CONFIG.__init__(size)
        
        # åˆ›å»ºæ¨¡å‹
        model = SignLLM(languages=["ASL"])
        
        # è®¡ç®—å®é™…å‚æ•°
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        print(f"  å®é™…å‚æ•°é‡: {total_params:,} ({total_params/1_000_000:.1f}M)")
        print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({trainable_params/1_000_000:.1f}M)")
        
        # ä¼°ç®—æ˜¾å­˜ä½¿ç”¨ (ç²—ç•¥)
        model_size_mb = total_params * 4 / (1024 * 1024)  # å‡è®¾float32
        print(f"  æ¨¡å‹å¤§å°: {model_size_mb:.1f} MB")
        
        del model  # é‡Šæ”¾å†…å­˜
        
    print("\nğŸ’¡ å»ºè®®:")
    print("  - tiny: å¿«é€ŸåŸå‹å¼€å‘å’Œè°ƒè¯•")
    print("  - small: å¹³è¡¡æ€§èƒ½å’Œé€Ÿåº¦")
    print("  - medium: æ›´å¥½çš„æ€§èƒ½")
    print("  - large: æœ€ä½³æ€§èƒ½ï¼ˆéœ€è¦æ›´å¤šèµ„æºï¼‰")

if __name__ == "__main__":
    test_model_sizes() 