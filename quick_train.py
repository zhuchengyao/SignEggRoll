#!/usr/bin/env python3
"""
å¿«é€Ÿè®­ç»ƒè„šæœ¬ - ç”¨äºæµ‹è¯•ASL text-to-poseç³»ç»Ÿ
"""

import torch
import torch.nn.functional as F
from torch.optim import AdamW
import numpy as np
import os
from tqdm import tqdm
from text2video_model import PoseUNet1D, TextToPoseDiffusion
from video_dataset import create_asl_dataloader
import matplotlib.pyplot as plt

def quick_test():
    """å¿«é€Ÿæµ‹è¯•æ•°æ®åŠ è½½å’Œæ¨¡å‹å‰å‘ä¼ æ’­"""
    print("ğŸš€ å¼€å§‹å¿«é€Ÿæµ‹è¯•...")
    
    # æ£€æŸ¥è®¾å¤‡
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æ•°æ®è·¯å¾„
    data_dir = "datasets/signllm_training_data/ASL/dev"
    if not os.path.exists(data_dir):
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return False
    
    print(f"âœ… æ•°æ®ç›®å½•å­˜åœ¨: {data_dir}")
    
    try:
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆå°æ‰¹æ¬¡æµ‹è¯•ï¼‰
        print("ğŸ“Š åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
        dataloader, dataset = create_asl_dataloader(
            data_dir=data_dir,
            batch_size=2,
            shuffle=False,
            num_workers=0,  # é¿å…multiprocessingé—®é¢˜
            num_frames=30,  # è¾ƒçŸ­åºåˆ—ç”¨äºæµ‹è¯•
            max_samples=5,  # åªåŠ è½½5ä¸ªæ ·æœ¬
            normalize=True,
            augment=False
        )
        
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸï¼Œæ•°æ®é›†å¤§å°: {len(dataset)}")
        
        # æµ‹è¯•ä¸€ä¸ªæ‰¹æ¬¡
        print("ğŸ” æµ‹è¯•æ•°æ®æ‰¹æ¬¡...")
        for batch in dataloader:
            pose_sequences = batch['pose_sequences']
            captions = batch['captions']
            print(f"   å§¿æ€åºåˆ—å½¢çŠ¶: {pose_sequences.shape}")
            print(f"   æ–‡æœ¬æ•°é‡: {len(captions)}")
            print(f"   ç¤ºä¾‹æ–‡æœ¬: {captions[0][:50]}...")
            break
        
        # åˆ›å»ºå°æ¨¡å‹è¿›è¡Œæµ‹è¯•
        print("ğŸ—ï¸ åˆ›å»ºæ¨¡å‹...")
        model = PoseUNet1D(
            pose_dim=120,
            model_channels=64,  # å°ä¸€äº›çš„æ¨¡å‹
            num_res_blocks=1,   # å‡å°‘å±‚æ•°
            num_frames=30,
            use_transformer=False  # å…ˆä¸ä½¿ç”¨transformer
        ).to(device)
        
        print(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œå‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        
        # åˆ›å»ºæ‰©æ•£è¿‡ç¨‹
        diffusion = TextToPoseDiffusion(num_timesteps=50)  # å‡å°‘æ—¶é—´æ­¥
        diffusion.betas = diffusion.betas.to(device)
        diffusion.alphas = diffusion.alphas.to(device)
        diffusion.alphas_cumprod = diffusion.alphas_cumprod.to(device)
        diffusion.alphas_cumprod_prev = diffusion.alphas_cumprod_prev.to(device)
        diffusion.posterior_variance = diffusion.posterior_variance.to(device)
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        print("âš¡ æµ‹è¯•å‰å‘ä¼ æ’­...")
        pose_sequences = pose_sequences.to(device)
        timesteps = torch.randint(0, 50, (pose_sequences.shape[0],)).to(device)
        
        with torch.no_grad():
            output = model(pose_sequences, timesteps, captions)
            print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸï¼è¾“å‡ºå½¢çŠ¶: {output.shape}")
        
        # æµ‹è¯•æŸå¤±è®¡ç®—
        print("ğŸ“‰ æµ‹è¯•æŸå¤±è®¡ç®—...")
        loss = diffusion.p_losses(model, pose_sequences, timesteps, captions)
        print(f"âœ… æŸå¤±è®¡ç®—æˆåŠŸï¼æŸå¤±å€¼: {loss.item():.6f}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def quick_train():
    """å¿«é€Ÿè®­ç»ƒå‡ ä¸ªæ­¥éª¤"""
    print("ğŸ‹ï¸ å¼€å§‹å¿«é€Ÿè®­ç»ƒ...")
    
    if not quick_test():
        print("âŒ åŸºç¡€æµ‹è¯•å¤±è´¥ï¼Œæ— æ³•å¼€å§‹è®­ç»ƒ")
        return
    
    # è®¾ç½®
    device = "cuda" if torch.cuda.is_available() else "cpu"
    data_dir = "datasets/signllm_training_data/ASL/dev"
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataloader, dataset = create_asl_dataloader(
        data_dir=data_dir,
        batch_size=2,
        shuffle=True,
        num_workers=0,
        num_frames=30,
        max_samples=10,  # åªç”¨10ä¸ªæ ·æœ¬è®­ç»ƒ
        normalize=True,
        augment=False
    )
    
    # åˆ›å»ºæ¨¡å‹
    model = PoseUNet1D(
        pose_dim=120,
        model_channels=64,
        num_res_blocks=1,
        num_frames=30,
        use_transformer=False
    ).to(device)
    
    # åˆ›å»ºæ‰©æ•£è¿‡ç¨‹
    diffusion = TextToPoseDiffusion(num_timesteps=50)
    diffusion.betas = diffusion.betas.to(device)
    diffusion.alphas = diffusion.alphas.to(device)
    diffusion.alphas_cumprod = diffusion.alphas_cumprod.to(device)
    diffusion.alphas_cumprod_prev = diffusion.alphas_cumprod_prev.to(device)
    diffusion.posterior_variance = diffusion.posterior_variance.to(device)
    
    # ä¼˜åŒ–å™¨
    optimizer = AdamW(model.parameters(), lr=1e-3)
    
    # è®­ç»ƒå‡ ä¸ªæ­¥éª¤
    model.train()
    losses = []
    
    print("å¼€å§‹è®­ç»ƒ...")
    for epoch in range(3):  # åªè®­ç»ƒ3ä¸ªepoch
        epoch_losses = []
        pbar = tqdm(dataloader, desc=f'Epoch {epoch+1}/3')
        
        for batch in pbar:
            pose_sequences = batch['pose_sequences'].to(device)
            captions = batch['captions']
            
            # éšæœºæ—¶é—´æ­¥
            t = torch.randint(0, 50, (pose_sequences.shape[0],)).to(device)
            
            # è®¡ç®—æŸå¤±
            loss = diffusion.p_losses(model, pose_sequences, t, captions)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            epoch_losses.append(loss.item())
            pbar.set_postfix({'loss': f'{loss.item():.6f}'})
        
        avg_loss = np.mean(epoch_losses)
        losses.append(avg_loss)
        print(f"Epoch {epoch+1} å¹³å‡æŸå¤±: {avg_loss:.6f}")
    
    # ç»˜åˆ¶æŸå¤±æ›²çº¿
    plt.figure(figsize=(8, 6))
    plt.plot(losses, 'b-o')
    plt.title('è®­ç»ƒæŸå¤±')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.grid(True)
    plt.savefig('training_loss.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    print("âœ… å¿«é€Ÿè®­ç»ƒå®Œæˆï¼")
    print("æ¨¡å‹å·²ç»æˆåŠŸè¿è¡Œï¼Œç°åœ¨å¯ä»¥å¼€å§‹å®Œæ•´è®­ç»ƒã€‚")
    
    # ä¿å­˜æ¨¡å‹
    checkpoint = {
        'model_state_dict': model.state_dict(),
        'config': {
            'pose_dim': 120,
            'model_channels': 64,
            'num_frames': 30,
            'num_timesteps': 50
        }
    }
    torch.save(checkpoint, 'quick_test_model.pth')
    print("æ¨¡å‹å·²ä¿å­˜ä¸º quick_test_model.pth")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == 'test':
        quick_test()
    else:
        quick_train() 