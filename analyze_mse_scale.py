"""
åˆ†æMSEé‡çº§çš„è„šæœ¬
ç”¨äºç†è§£ä¸ºä»€ä¹ˆè®­ç»ƒå¼€å§‹æ—¶MSEå°±åœ¨0.001çº§åˆ«
"""
import torch
import numpy as np
import json
import os
from transformers import BertTokenizer
from text_to_pose_trainer import TextToPoseDataset

def analyze_data_statistics():
    """åˆ†æè®­ç»ƒæ•°æ®çš„ç»Ÿè®¡ä¿¡æ¯"""
    print("ğŸ” åˆ†æè®­ç»ƒæ•°æ®çš„ç»Ÿè®¡ä¿¡æ¯...")
    
    # ä½¿ç”¨ä¸è®­ç»ƒç›¸åŒçš„æ•°æ®è·¯å¾„
    data_paths = [
        "datasets/signllm_training_data/ASL/dev/dev_fz6XzPxdo-0_3-5-rgb_front",
        "datasets/signllm_training_data/ASL/dev/dev_fz6XzPxdo-0_2-5-rgb_front", 
        "datasets/signllm_training_data/ASL/dev/dev_fz6XzPxdo-0_17-5-rgb_front",
    ]
    
    # åˆ†æåŸå§‹æ•°æ®èŒƒå›´
    print("\n1. åŸå§‹æ•°æ®åˆ†æ:")
    analyze_raw_data(data_paths)
    
    # åˆ†æå¤„ç†åçš„æ•°æ®
    print("\n2. å¤„ç†åæ•°æ®åˆ†æ:")
    tokenizer = BertTokenizer.from_pretrained("bert-large-uncased")
    dataset = TextToPoseDataset(data_paths, tokenizer, max_length=512, max_sequence_length=400)
    analyze_processed_data(dataset)
    
    # è®¡ç®—ç†è®ºMSEèŒƒå›´
    print("\n3. MSEé‡çº§åˆ†æ:")
    analyze_mse_scale(dataset)

def analyze_raw_data(data_paths):
    """åˆ†æåŸå§‹JSONæ•°æ®"""
    all_coords = []
    
    for path in data_paths:
        pose_path = os.path.join(path, "pose.json")
        if os.path.exists(pose_path):
            with open(pose_path, 'r') as f:
                pose_data = json.load(f)
            
            print(f"\næ•°æ®è·¯å¾„: {path}")
            
            for frame_idx, pose in enumerate(pose_data['poses']):
                # æ”¶é›†æ‰€æœ‰åæ ‡
                coords = []
                
                # èº«ä½“å…³é”®ç‚¹
                body_coords = pose['pose_keypoints_2d']
                for i in range(0, len(body_coords), 3):
                    x, y = body_coords[i], body_coords[i+1]
                    if abs(x) > 1e-6 or abs(y) > 1e-6:  # éé›¶ç‚¹
                        coords.extend([x, y])
                
                # å·¦æ‰‹å…³é”®ç‚¹
                left_hand_coords = pose['hand_left_keypoints_2d']
                for i in range(0, len(left_hand_coords), 3):
                    x, y = left_hand_coords[i], left_hand_coords[i+1]
                    if abs(x) > 1e-6 or abs(y) > 1e-6:
                        coords.extend([x, y])
                
                # å³æ‰‹å…³é”®ç‚¹  
                right_hand_coords = pose['hand_right_keypoints_2d']
                for i in range(0, len(right_hand_coords), 3):
                    x, y = right_hand_coords[i], right_hand_coords[i+1]
                    if abs(x) > 1e-6 or abs(y) > 1e-6:
                        coords.extend([x, y])
                
                all_coords.extend(coords)
                
                # æ˜¾ç¤ºå‰å‡ å¸§çš„ç»Ÿè®¡
                if frame_idx < 3 and coords:
                    coords_array = np.array(coords)
                    print(f"  å¸§ {frame_idx}: èŒƒå›´ [{coords_array.min():.3f}, {coords_array.max():.3f}], "
                          f"å‡å€¼ {coords_array.mean():.3f}, æ ‡å‡†å·® {coords_array.std():.3f}")
    
    if all_coords:
        all_coords = np.array(all_coords)
        print(f"\nåŸå§‹æ•°æ®æ€»ä½“ç»Ÿè®¡:")
        print(f"  æ•°æ®ç‚¹æ•°: {len(all_coords)}")
        print(f"  æ•°å€¼èŒƒå›´: [{all_coords.min():.3f}, {all_coords.max():.3f}]")
        print(f"  å‡å€¼: {all_coords.mean():.3f}")
        print(f"  æ ‡å‡†å·®: {all_coords.std():.3f}")
        print(f"  ä¸­ä½æ•°: {np.median(all_coords):.3f}")

def analyze_processed_data(dataset):
    """åˆ†æå¤„ç†åçš„æ•°æ®"""
    print(f"æ•°æ®é›†å¤§å°: {len(dataset)}")
    
    all_keypoints = []
    for i in range(len(dataset)):
        item = dataset[i]
        keypoints = item['keypoints_sequence'].numpy()  # [400, 150]
        all_keypoints.append(keypoints)
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    all_data = np.concatenate(all_keypoints, axis=0)  # [N, 150]
    print(f"æ€»æ•°æ®å½¢çŠ¶: {all_data.shape}")
    
    # åˆ†æéé›¶æ•°æ®
    non_zero_mask = np.abs(all_data) > 1e-6
    non_zero_data = all_data[non_zero_mask]
    
    print(f"\nå¤„ç†åæ•°æ®ç»Ÿè®¡:")
    print(f"  æ€»æ•°æ®ç‚¹: {all_data.size:,}")
    print(f"  éé›¶æ•°æ®ç‚¹: {len(non_zero_data):,} ({100*len(non_zero_data)/all_data.size:.1f}%)")
    print(f"  æ•°å€¼èŒƒå›´: [{all_data.min():.6f}, {all_data.max():.6f}]")
    print(f"  éé›¶æ•°å€¼èŒƒå›´: [{non_zero_data.min():.6f}, {non_zero_data.max():.6f}]") if len(non_zero_data) > 0 else None
    print(f"  å‡å€¼: {all_data.mean():.6f}")
    print(f"  æ ‡å‡†å·®: {all_data.std():.6f}")
    print(f"  éé›¶å‡å€¼: {non_zero_data.mean():.6f}") if len(non_zero_data) > 0 else None
    print(f"  éé›¶æ ‡å‡†å·®: {non_zero_data.std():.6f}") if len(non_zero_data) > 0 else None

def analyze_mse_scale(dataset):
    """åˆ†æMSEé‡çº§"""
    # è·å–ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®
    item = dataset[0]
    keypoints = item['keypoints_sequence'].numpy()  # [400, 150]
    
    print(f"å•ä¸ªæ ·æœ¬å½¢çŠ¶: {keypoints.shape}")
    print(f"æ€»å…ƒç´ æ•°: {keypoints.size:,}")
    
    # è®¡ç®—æ•°æ®çš„ç»Ÿè®¡ä¿¡æ¯
    data_std = keypoints.std()
    data_mean = np.abs(keypoints).mean()
    data_max = np.abs(keypoints).max()
    
    print(f"\næ•°æ®ç»Ÿè®¡:")
    print(f"  æ ‡å‡†å·®: {data_std:.6f}")
    print(f"  ç»å¯¹å€¼å‡å€¼: {data_mean:.6f}")
    print(f"  ç»å¯¹å€¼æœ€å¤§å€¼: {data_max:.6f}")
    
    # æ¨¡æ‹Ÿéšæœºåˆå§‹åŒ–çš„é¢„æµ‹
    print(f"\næ¨¡æ‹ŸMSEè®¡ç®—:")
    
    # 1. å¦‚æœé¢„æµ‹å€¼æ˜¯éšæœºåˆå§‹åŒ–ï¼ˆé€šå¸¸åœ¨[-1, 1]æˆ–æ›´å°èŒƒå›´ï¼‰
    random_pred = np.random.normal(0, 0.1, keypoints.shape)  # æ ‡å‡†å·®0.1çš„éšæœºé¢„æµ‹
    mse_random = np.mean((random_pred - keypoints) ** 2)
    print(f"  éšæœºé¢„æµ‹ MSE: {mse_random:.6f}")
    
    # 2. å¦‚æœé¢„æµ‹å€¼æ˜¯å…¨é›¶
    zero_pred = np.zeros_like(keypoints)
    mse_zero = np.mean((zero_pred - keypoints) ** 2)
    print(f"  å…¨é›¶é¢„æµ‹ MSE: {mse_zero:.6f}")
    
    # 3. å¦‚æœé¢„æµ‹å€¼æ˜¯å‡å€¼
    mean_pred = np.full_like(keypoints, keypoints.mean())
    mse_mean = np.mean((mean_pred - keypoints) ** 2)
    print(f"  å‡å€¼é¢„æµ‹ MSE: {mse_mean:.6f}")
    
    # 4. ç†è®ºåˆ†æ
    print(f"\nç†è®ºåˆ†æ:")
    print(f"  å¦‚æœæ•°æ®èŒƒå›´åœ¨ [-1, 1]ï¼Œéšæœºé¢„æµ‹çš„æœŸæœ›MSEçº¦ä¸º: {(2**2)/3:.6f}")
    print(f"  å¦‚æœæ•°æ®æ ‡å‡†å·®ä¸º {data_std:.3f}ï¼Œæ–¹å·®ä¸º: {data_std**2:.6f}")
    print(f"  è€ƒè™‘åˆ°æ•°æ®ç¨€ç–æ€§ï¼ˆå¾ˆå¤šé›¶å€¼ï¼‰ï¼Œå®é™…MSEä¼šæ›´å°")
    
    # 5. æ£€æŸ¥æ•°æ®æ˜¯å¦å·²ç»æ ‡å‡†åŒ–
    non_zero_data = keypoints[np.abs(keypoints) > 1e-6]
    if len(non_zero_data) > 0:
        print(f"\næ•°æ®æ ‡å‡†åŒ–æ£€æŸ¥:")
        print(f"  éé›¶æ•°æ®æ˜¯å¦åœ¨[-1,1]èŒƒå›´: {non_zero_data.min() >= -1 and non_zero_data.max() <= 1}")
        print(f"  éé›¶æ•°æ®æ˜¯å¦æ¥è¿‘æ ‡å‡†æ­£æ€åˆ†å¸ƒ: å‡å€¼={non_zero_data.mean():.3f}, æ ‡å‡†å·®={non_zero_data.std():.3f}")

def simulate_training_mse():
    """æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ä¸­çš„MSEå˜åŒ–"""
    print(f"\n4. æ¨¡æ‹Ÿè®­ç»ƒMSEå˜åŒ–:")
    
    # æ¨¡æ‹ŸçœŸå®æ•°æ®çš„ç‰¹å¾
    # å‡è®¾æ•°æ®å·²ç»æ ‡å‡†åŒ–åˆ°è¾ƒå°èŒƒå›´
    np.random.seed(42)
    target = np.random.normal(0, 0.3, (400, 150))  # ç›®æ ‡æ•°æ®
    target[np.random.rand(*target.shape) < 0.7] = 0  # 70%çš„æ•°æ®ä¸º0ï¼ˆç¨€ç–æ€§ï¼‰
    
    print(f"æ¨¡æ‹Ÿç›®æ ‡æ•°æ®ç»Ÿè®¡:")
    print(f"  å½¢çŠ¶: {target.shape}")
    print(f"  èŒƒå›´: [{target.min():.3f}, {target.max():.3f}]")
    print(f"  éé›¶æ¯”ä¾‹: {(np.abs(target) > 1e-6).mean():.1%}")
    print(f"  æ ‡å‡†å·®: {target.std():.6f}")
    
    # æ¨¡æ‹Ÿä¸åŒè®­ç»ƒé˜¶æ®µçš„é¢„æµ‹
    scenarios = [
        ("éšæœºåˆå§‹åŒ–", np.random.normal(0, 0.1, target.shape)),
        ("è½»å¾®è®­ç»ƒå", target + np.random.normal(0, 0.2, target.shape)),
        ("è¾ƒå¥½è®­ç»ƒå", target + np.random.normal(0, 0.05, target.shape)),
        ("æ”¶æ•›çŠ¶æ€", target + np.random.normal(0, 0.01, target.shape)),
    ]
    
    print(f"\nä¸åŒè®­ç»ƒé˜¶æ®µçš„MSE:")
    for name, pred in scenarios:
        mse = np.mean((pred - target) ** 2)
        print(f"  {name}: MSE = {mse:.6f}")

def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("MSE é‡çº§åˆ†ææŠ¥å‘Š")
    print("="*60)
    
    # æ£€æŸ¥æ•°æ®è·¯å¾„æ˜¯å¦å­˜åœ¨
    data_paths = [
        "datasets/signllm_training_data/ASL/dev/dev_fz6XzPxdo-0_3-5-rgb_front",
        "datasets/signllm_training_data/ASL/dev/dev_fz6XzPxdo-0_2-5-rgb_front",
        "datasets/signllm_training_data/ASL/dev/dev_fz6XzPxdo-0_17-5-rgb_front",
    ]
    
    missing_paths = [path for path in data_paths if not os.path.exists(path)]
    if missing_paths:
        print(f"âŒ ç¼ºå°‘æ•°æ®è·¯å¾„: {missing_paths}")
        print("ğŸ”„ ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œåˆ†æ...")
        simulate_training_mse()
    else:
        print("âœ… æ•°æ®è·¯å¾„å­˜åœ¨ï¼Œå¼€å§‹åˆ†æ...")
        analyze_data_statistics()
    
    simulate_training_mse()
    
    print(f"\n" + "="*60)
    print("ç»“è®º:")
    print("="*60)
    print("1. MSE = 0.001 é‡çº§æ˜¯åˆç†çš„ï¼Œå› ä¸º:")
    print("   - æ•°æ®é€šå¸¸å·²ç»æ ‡å‡†åŒ–åˆ°è¾ƒå°èŒƒå›´ï¼ˆå¦‚[-1,1]æˆ–æ›´å°ï¼‰")
    print("   - æ•°æ®å…·æœ‰ç¨€ç–æ€§ï¼ˆå¾ˆå¤šå…³é”®ç‚¹ä¸º0ï¼‰")
    print("   - MSEæ˜¯å¯¹60,000ä¸ªæ•°å€¼æ±‚å¹³å‡ï¼Œä¼šé™ä½æ•´ä½“é‡çº§")
    print("   - éšæœºåˆå§‹åŒ–çš„ç½‘ç»œé¢„æµ‹å€¼é€šå¸¸ä¹Ÿåœ¨è¾ƒå°èŒƒå›´å†…")
    
    print(f"\n2. é¢„æœŸçš„MSEå˜åŒ–:")
    print("   - åˆå§‹: 0.001 - 0.01 ï¼ˆéšæœºé¢„æµ‹ï¼‰")
    print("   - è®­ç»ƒä¸­: é€æ¸é™ä½")
    print("   - è¿‡æ‹Ÿåˆç›®æ ‡: < 1e-4 æˆ– 1e-7")
    
    print(f"\n3. å¦‚æœMSEå¼‚å¸¸:")
    print("   - å¦‚æœåˆå§‹MSE > 0.1: å¯èƒ½æ•°æ®æœªæ ‡å‡†åŒ–æˆ–ç½‘ç»œåˆå§‹åŒ–æœ‰é—®é¢˜")
    print("   - å¦‚æœMSEä¸ä¸‹é™: å¯èƒ½å­¦ä¹ ç‡å¤ªå°æˆ–æ¢¯åº¦æ¶ˆå¤±")
    print("   - å¦‚æœMSEä¸‹é™å¤ªæ…¢: å¯èƒ½éœ€è¦è°ƒæ•´ç½‘ç»œç»“æ„æˆ–å­¦ä¹ ç‡")

if __name__ == "__main__":
    main() 