#!/usr/bin/env python3
"""
æ”¹è¿›çš„ SignLLM è¯„ä¼°ç³»ç»Ÿ
è§£å†³ä½™å¼¦ç›¸ä¼¼åº¦ä¸é€‚åˆå§¿æ€ä»»åŠ¡çš„é—®é¢˜
"""

import numpy as np
import torch
from typing import List, Dict, Tuple, Optional
from scipy.spatial.distance import euclidean
from fastdtw import fastdtw
from sklearn.metrics import mean_squared_error, mean_absolute_error
import logging

logger = logging.getLogger(__name__)


class ImprovedSignLLMEvaluator:
    """æ”¹è¿›çš„SignLLMè¯„ä¼°å™¨ - ä½¿ç”¨æ›´é€‚åˆå§¿æ€çš„è¯„ä¼°æŒ‡æ ‡"""
    
    def __init__(self):
        pass
    
    def evaluate_poses(self, predictions: List[np.ndarray], targets: List[np.ndarray]) -> Dict[str, float]:
        """è¯„ä¼°å§¿æ€ç”Ÿæˆè´¨é‡ - ä½¿ç”¨å¤šç§äº’è¡¥çš„æŒ‡æ ‡"""
        if len(predictions) != len(targets):
            raise ValueError("Predictions and targets must have the same length")
        
        metrics = {}
        
        # 1. DTWè·ç¦»ï¼ˆæ—¶é—´åºåˆ—ç›¸ä¼¼æ€§ï¼‰
        dtw_scores = []
        for pred, target in zip(predictions, targets):
            dtw_distance, _ = fastdtw(pred, target, dist=euclidean)
            dtw_scores.append(dtw_distance)
        
        metrics["dtw_distance"] = np.mean(dtw_scores)
        metrics["dtw_score"] = 1.0 / (1.0 + metrics["dtw_distance"])
        
        # 2. åŸºç¡€è·ç¦»æŒ‡æ ‡
        all_pred = np.concatenate([p.flatten() for p in predictions])
        all_target = np.concatenate([t.flatten() for t in targets])
        
        metrics["mse"] = mean_squared_error(all_target, all_pred)
        metrics["mae"] = mean_absolute_error(all_target, all_pred)
        metrics["rmse"] = np.sqrt(metrics["mse"])
        
        # 3. æ”¹è¿›çš„å§¿æ€ç›¸ä¼¼åº¦æŒ‡æ ‡
        pose_similarities = []
        euclidean_similarities = []
        weighted_similarities = []
        
        for pred, target in zip(predictions, targets):
            # å¤šç§ç›¸ä¼¼åº¦è®¡ç®—
            cosine_sim = self._calculate_cosine_similarity(pred, target)
            euclidean_sim = self._calculate_euclidean_similarity(pred, target)
            weighted_sim = self._calculate_weighted_pose_similarity(pred, target)
            
            pose_similarities.append(cosine_sim)
            euclidean_similarities.append(euclidean_sim)
            weighted_similarities.append(weighted_sim)
        
        metrics["cosine_similarity"] = np.mean(pose_similarities)  # åŸæ¥çš„æŒ‡æ ‡
        metrics["euclidean_similarity"] = np.mean(euclidean_similarities)  # åŸºäºè·ç¦»
        metrics["weighted_similarity"] = np.mean(weighted_similarities)  # åŠ æƒæŒ‡æ ‡
        metrics["pose_similarity"] = np.mean(weighted_similarities)  # ä¸»è¦æŒ‡æ ‡
        
        # 4. è¿åŠ¨å¹³æ»‘åº¦
        motion_smoothness = []
        for pred in predictions:
            smoothness = self._calculate_motion_smoothness(pred)
            motion_smoothness.append(smoothness)
        
        metrics["motion_smoothness"] = np.mean(motion_smoothness)
        
        # 5. å…³é”®ç‚¹å‡†ç¡®æ€§ï¼ˆåˆ†éƒ¨ä½è¯„ä¼°ï¼‰
        keypoint_metrics = self._evaluate_keypoint_accuracy(predictions, targets)
        metrics.update(keypoint_metrics)
        
        return metrics
    
    def _calculate_cosine_similarity(self, pred: np.ndarray, target: np.ndarray) -> float:
        """åŸæ¥çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆä¿ç•™ç”¨äºå¯¹æ¯”ï¼‰"""
        min_len = min(len(pred), len(target))
        pred = pred[:min_len]
        target = target[:min_len]
        
        frame_similarities = []
        for p_frame, t_frame in zip(pred, target):
            p_norm = np.linalg.norm(p_frame)
            t_norm = np.linalg.norm(t_frame)
            
            if p_norm == 0 or t_norm == 0:
                similarity = 0.0
            else:
                similarity = np.dot(p_frame, t_frame) / (p_norm * t_norm)
            
            frame_similarities.append(max(0.0, similarity))
        
        return np.mean(frame_similarities)
    
    def _calculate_euclidean_similarity(self, pred: np.ndarray, target: np.ndarray) -> float:
        """åŸºäºæ¬§æ°è·ç¦»çš„ç›¸ä¼¼åº¦ï¼ˆæ›´é€‚åˆå§¿æ€ä»»åŠ¡ï¼‰"""
        min_len = min(len(pred), len(target))
        pred = pred[:min_len]
        target = target[:min_len]
        
        frame_similarities = []
        for p_frame, t_frame in zip(pred, target):
            # è®¡ç®—æ¬§æ°è·ç¦»ï¼Œè½¬æ¢ä¸ºç›¸ä¼¼åº¦
            distance = np.linalg.norm(p_frame - t_frame)
            similarity = 1.0 / (1.0 + distance)  # è·ç¦»è¶Šå°ï¼Œç›¸ä¼¼åº¦è¶Šé«˜
            frame_similarities.append(similarity)
        
        return np.mean(frame_similarities)
    
    def _calculate_weighted_pose_similarity(self, pred: np.ndarray, target: np.ndarray) -> float:
        """åŠ æƒå§¿æ€ç›¸ä¼¼åº¦ - é‡è¦éƒ¨ä½æƒé‡æ›´é«˜"""
        min_len = min(len(pred), len(target))
        pred = pred[:min_len]
        target = target[:min_len]
        
        # é‡å¡‘ä¸ºå…³é”®ç‚¹æ ¼å¼ [seq_len, 50, 3]
        pred_joints = pred.reshape(min_len, 50, 3)
        target_joints = target.reshape(min_len, 50, 3)
        
        # å®šä¹‰å…³é”®ç‚¹æƒé‡
        weights = np.ones(50)
        weights[:8] = 2.0      # ä¸Šèº«å…³é”®ç‚¹æƒé‡æ›´é«˜
        weights[8:29] = 1.5    # å·¦æ‰‹
        weights[29:50] = 1.5   # å³æ‰‹
        weights[[0, 1, 4, 7]] = 3.0  # å¤´éƒ¨ã€é¢ˆéƒ¨ã€æ‰‹è…•ç­‰å…³é”®ç‚¹
        
        frame_similarities = []
        for p_frame, t_frame in zip(pred_joints, target_joints):
            # è®¡ç®—æ¯ä¸ªå…³é”®ç‚¹çš„è·ç¦»
            joint_distances = np.linalg.norm(p_frame - t_frame, axis=1)
            
            # åº”ç”¨æƒé‡
            weighted_distances = joint_distances * weights
            
            # è½¬æ¢ä¸ºç›¸ä¼¼åº¦
            similarities = 1.0 / (1.0 + weighted_distances)
            
            # åŠ æƒå¹³å‡
            frame_similarity = np.average(similarities, weights=weights)
            frame_similarities.append(frame_similarity)
        
        return np.mean(frame_similarities)
    
    def _evaluate_keypoint_accuracy(self, predictions: List[np.ndarray], 
                                   targets: List[np.ndarray]) -> Dict[str, float]:
        """åˆ†éƒ¨ä½è¯„ä¼°å…³é”®ç‚¹å‡†ç¡®æ€§"""
        metrics = {}
        
        # å®šä¹‰éƒ¨ä½ç´¢å¼•
        body_parts = {
            'upper_body': list(range(0, 8)),      # ä¸Šèº«
            'left_hand': list(range(8, 29)),      # å·¦æ‰‹
            'right_hand': list(range(29, 50)),    # å³æ‰‹
            'key_joints': [0, 1, 4, 7]            # å…³é”®å…³èŠ‚
        }
        
        for part_name, indices in body_parts.items():
            part_errors = []
            
            for pred, target in zip(predictions, targets):
                min_len = min(len(pred), len(target))
                pred_part = pred[:min_len].reshape(min_len, 50, 3)[:, indices, :]
                target_part = target[:min_len].reshape(min_len, 50, 3)[:, indices, :]
                
                # è®¡ç®—è¯¥éƒ¨ä½çš„å¹³å‡è¯¯å·®
                error = np.mean(np.linalg.norm(pred_part - target_part, axis=2))
                part_errors.append(error)
            
            metrics[f"{part_name}_error"] = np.mean(part_errors)
            metrics[f"{part_name}_similarity"] = 1.0 / (1.0 + metrics[f"{part_name}_error"])
        
        return metrics
    
    def _calculate_motion_smoothness(self, poses: np.ndarray) -> float:
        """è®¡ç®—è¿åŠ¨å¹³æ»‘åº¦"""
        if len(poses) < 2:
            return 1.0
        
        # è®¡ç®—ç›¸é‚»å¸§ä¹‹é—´çš„å·®å¼‚
        diffs = []
        for i in range(1, len(poses)):
            diff = np.linalg.norm(poses[i] - poses[i-1])
            diffs.append(diff)
        
        # å¹³æ»‘åº¦ = 1 / (1 + å¹³å‡å·®å¼‚)
        avg_diff = np.mean(diffs)
        smoothness = 1.0 / (1.0 + avg_diff)
        
        return smoothness


def compare_evaluation_methods(predictions: List[np.ndarray], targets: List[np.ndarray]):
    """æ¯”è¾ƒä¸åŒè¯„ä¼°æ–¹æ³•çš„ç»“æœ"""
    from evaluation import SignLLMEvaluator
    
    # åŸå§‹è¯„ä¼°å™¨
    original_evaluator = SignLLMEvaluator()
    original_metrics = original_evaluator.evaluate_poses(predictions, targets)
    
    # æ”¹è¿›è¯„ä¼°å™¨
    improved_evaluator = ImprovedSignLLMEvaluator()
    improved_metrics = improved_evaluator.evaluate_poses(predictions, targets)
    
    print("ğŸ“Š è¯„ä¼°æ–¹æ³•å¯¹æ¯”:")
    print("=" * 60)
    
    print("\nğŸ”¸ åŸå§‹è¯„ä¼°å™¨:")
    for key, value in original_metrics.items():
        if 'similarity' in key:
            print(f"  {key:25s}: {value:.4f}")
    
    print("\nğŸ”¹ æ”¹è¿›è¯„ä¼°å™¨:")
    for key, value in improved_metrics.items():
        if 'similarity' in key or 'error' in key:
            print(f"  {key:25s}: {value:.4f}")
    
    print("\nğŸ’¡ å»ºè®®:")
    print("  - ä½¿ç”¨ weighted_similarity ä½œä¸ºä¸»è¦æŒ‡æ ‡")
    print("  - euclidean_similarity æ›´é€‚åˆå§¿æ€ä»»åŠ¡")
    print("  - åˆ†éƒ¨ä½è¯¯å·®å¸®åŠ©è¯Šæ–­é—®é¢˜")
    
    return original_metrics, improved_metrics


if __name__ == "__main__":
    # æµ‹è¯•è¯„ä¼°å™¨
    evaluator = ImprovedSignLLMEvaluator()
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    predictions = [np.random.randn(50, 150) for _ in range(5)]
    targets = [np.random.randn(50, 150) for _ in range(5)]
    
    # æµ‹è¯•æ”¹è¿›çš„è¯„ä¼°
    metrics = evaluator.evaluate_poses(predictions, targets)
    print("æ”¹è¿›çš„å§¿æ€è¯„ä¼°æŒ‡æ ‡:")
    for key, value in metrics.items():
        print(f"  {key}: {value:.4f}")
    
    print("\n" + "="*50)
    compare_evaluation_methods(predictions, targets) 